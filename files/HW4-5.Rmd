---
title: " IE 360 HW 4-5: Trendyol Products Sales Forecasting "
author: " Seyma Cakir - 2017402024 Oya Hoban - 2018402150 Ozlem SENEL - 2017402117 "
date:  " 2021-07-03 " 
output:
 html_document:
   toc: true
   toc_float: true
   smooth_scroll : true
---

````{r }
knitr::opts_chunk$set( echo = FALSE, warning = FALSE, message = FALSE)
```


```{r setup2, include=FALSE}
library(jsonlite)
library(httr)
library(data.table)
library(lubridate)
library(dplyr)
library(ggplot2)
library(knitr)
library(tidyr)
library(tidyverse)
library(scales)
library(ggcorrplot)
library(forecast)
library(urca)
library(zoo)
library(reshape)
library(GGally)
library(PerformanceAnalytics)
library(readxl)
library(RColorBrewer)
library(colorspace)
library(tibble)
library(fpp)
library(xts)
library(gridExtra)
library(MLmetrics)
library(readr)

get_token <- function(username, password, url_site){
  
  post_body = list(username=username,password=password)
  post_url_string = paste0(url_site,'/token/')
  result = POST(post_url_string, body = post_body)
  
  # error handling (wrong credentials)
  if(result$status_code==400){
    print('Check your credentials')
    return(0)
  }
  else if (result$status_code==201){
    output = content(result)
    token = output$key
  }
  
  return(token)
}

get_data <- function(start_date='2020-03-20', token, url_site){
  
  post_body = list(start_date=start_date,username=username,password=password)
  post_url_string = paste0(url_site,'/dataset/')
  
  header = add_headers(c(Authorization=paste('Token',token,sep=' ')))
  result = GET(post_url_string, header, body = post_body)
  output = content(result)
  data = data.table::rbindlist(output)
  data[,event_date:=as.Date(event_date)]
  data = data[order(product_content_id,event_date)]
  return(data)
}


send_submission <- function(predictions, token, url_site, submit_now=F){
  
  format_check=check_format(predictions)
  if(!format_check){
    return(FALSE)
  }
  
  post_string="list("
  for(i in 1:nrow(predictions)){
    post_string=sprintf("%s'%s'=%s",post_string,predictions$product_content_id[i],predictions$forecast[i])
    if(i<nrow(predictions)){
      post_string=sprintf("%s,",post_string)
    } else {
      post_string=sprintf("%s)",post_string)
    }
  }
  
  submission = eval(parse(text=post_string))
  json_body = jsonlite::toJSON(submission, auto_unbox = TRUE)
  submission=list(submission=json_body)
  
  print(submission)
  
  if(!submit_now){
    print("You did not submit.")
    return(FALSE)      
  }
  
  
  header = add_headers(c(Authorization=paste('Token',token,sep=' ')))
  post_url_string = paste0(url_site,'/submission/')
  result = POST(post_url_string, header, body=submission)
  
  if (result$status_code==201){
    print("Successfully submitted. Below you can see the details of your submission")
  } else {
    print("Could not submit. Please check the error message below, contact the assistant if needed.")
  }
  
  print(content(result))
  
}

check_format <- function(predictions){
  
  if(is.data.frame(predictions) | is.data.frame(predictions)){
    if(all(c('product_content_id','forecast') %in% names(predictions))){
      if(is.numeric(predictions$forecast)){
        print("Format OK")
        return(TRUE)
      } else {
        print("forecast information is not numeric")
        return(FALSE)                
      }
    } else {
      print("Wrong column names. Please provide 'product_content_id' and 'forecast' columns")
      return(FALSE)
    }
    
  } else {
    print("Wrong format. Please provide data.frame or data.table object")
    return(FALSE)
  }
  
}

subm_url = 'http://46.101.163.177'

u_name = "Group3"
p_word = "LaQjwxkIGSGhBrRj"
submit_now = FALSE

username = u_name
password = p_word

token = get_token(username=u_name, password=p_word, url=subm_url)
data = get_data(token=token,url=subm_url)

predictions=unique(data[,list(product_content_id)])
predictions[,forecast:=2.3]

rawdata <- read.csv("C:/Users/DELL/Desktop/ProjectRawData.csv", header = TRUE,sep = ",")
rawdata <- as.data.table(rawdata)
colnames <- c( "price", "event_date", "product_content_id", "sold_count", "visit_count", "favored_count", "basket_count", "category_sold", "category_brand_sold", "category_visits", "ty_visits", "category_basket", "category_favored" )
rawdata <- rawdata[,..colnames]
rawdata <- rawdata[!is.na(sold_count)]
rawdata$event_date <- as.Date(rawdata$event_date, format = "%Y-%m-%d")
rawdata$product_content_id <- as.factor(rawdata$product_content_id)
data$product_content_id <- as.factor(data$product_content_id)

rawdata <- rbind(rawdata,data)

rawdata <- rawdata %>% distinct(event_date, product_content_id, .keep_all = TRUE )
rawdata <- rawdata %>% arrange(event_date)

product_id <- unique(rawdata$product_content_id)
product_id
products <- list()
for ( i in 1:length(product_id)){
  products[[i]] <- as.data.table(rawdata[product_content_id == product_content_id[i]])
}



```


# INTRODUCTION

In this assignment, it will be predicted some of products that are sold at an e-commerce platform called 'Trendyol'. The sold count will be examined for each product and data willbe decomposed. Then,  some forecasting strategies will be developed  and the best among them according to their weighted mean absolute errors will be picked. The data before 23 June 2021 will be train dataset for our models to learn and data from 24 June to 30 June 2021 will be test dataset. There are 9 products that it will be examined:

+ 85004 - La Roche Posay Face Cleanser
+ 4066298 - Sleepy Baby Wipes
+ 6676673 - Xiaomi Bluetooth Headphones
+ 7061886 - Fakir Vacuum Cleaner
+ 31515569 - TrendyolMilla Tights
+ 32737302 - TrendyolMilla Bikini Top
+ 32939029 - Oral-B Rechargeable ToothBrush
+ 48740784 - Altınyıldız Classics Jacket
+ 73318567 - TrendyolMilla Bikini Top

# PRODUCT 1 - La Roche Posay Face Cleanser

```{r product1, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE}
product1 <- products[[1]]
setDT(product1)

product1$price[product1$price == 0] <- mean(product1$price[product1$price !=0])
product1$basket_count[product1$basket_count == 0] <- mean(product1$basket_count[product1$basket_count !=0])
product1$favored_count[product1$favored_count == 0] <- mean(product1$favored_count[product1$favored_count !=0])
product1$sold_count[product1$sold_count == 0] <- mean(product1$sold_count[product1$sold_count !=0])
product1$visit_count[product1$visit_count == 0] <- mean(product1$visit_count[product1$visit_count !=0])
product1$category_favored[product1$category_favored == 0] <- mean(product1$category_favored[product1$category_favored !=0])
product1$category_sold[product1$category_sold == 0] <- mean(product1$category_sold[product1$category_sold !=0])
product1$category_visits[product1$category_visits == 0] <- mean(product1$category_visits[product1$category_visits !=0])
product1$category_basket[product1$category_basket == 0] <- mean(product1$category_basket[product1$category_basket !=0])
product1$category_brand_sold[product1$category_brand_sold == 0] <- mean(product1$category_brand_sold[product1$category_brand_sold !=0])
product1$ty_visits[product1$ty_visits == 0] <- mean(product1$ty_visits[product1$ty_visits !=0])

train <- product1[product1$event_date <= as.Date("2021-06-23")]
test <- product1[product1$event_date > as.Date("2021-06-23") & product1$event_date <= as.Date("2021-06-30")]

```

Before making alternative models,  it should be looked at the plot of data and examined the seasonalities and trend. First of all, it's clear that the variance of data is very large, from now on, it can be continued with the logarithm of sold count. Below, you can see the actual plot and the plot with the values that are taken logarithms. There is a slightly increasing trend, especially in the middle of the plot. There can't be seen any significant seasonality. To look further, there is a plot of 3 months of 2021 - March, April and May -. Again, the seasonality isn't very significant but it is seen that the data is higher in the begining of the month and decreases to the end of the month. It can be said that there is monthly seasonality.


```{r plot daily1, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE}

sold_log <- log(train$sold_count)
train <- cbind(train, sold_log)

ggplot(train,aes(x=event_date,y=sold_count)) + geom_line(color="brown2") +  theme_light()+labs(x = "Date",
       y = "Sales Quantity of the Product")

ggplot(train,aes(x=event_date,y=sold_log)) + geom_line(color="brown2") +  theme_light()+labs(x = "Date",
       y = "Log of Sales Quantity of the Product") + geom_smooth(color="black", linetype="longdash", se = FALSE)

ggplot(train[event_date<='2021-05-30' & event_date>='2021-03-01'] ,aes(x=event_date,y=sold_log)) + geom_line(color="darkolivegreen4") +  theme_light()+labs(x = "3 months of 2021",
       y = "Log of Sales Quantity of the Product")

```

Before decomposing, to make better decision, it can be looked to the autocorrelation plot of data. Below, it can be seen that there is a spike at lag 63. We can determine the frequency as 63 just because the seasonality wasn't very significant.

```{r plot acf1, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE}

plotacf <- ggAcf(train$sold_log, lag.max=80,  col = "coral4") + theme_light()
grid.arrange(plotacf)

```

Now, the data will be decomposed in order to make random series."Additive" type of decomposing will be used because the variance of data is very low due to switching to logarithm. Plots of deseasonalized and the random series their autocorrelations can be seen below.

```{r decomposition1, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE}

ts_train<-ts(train$sold_log,freq=63, start=c(1,1))
ts_train_dec<-decompose(ts_train,type="additive")
plot(ts_train_dec)

seasonal<- ts_train_dec$seasonal
deseasonalize<-ts_train-seasonal

tsdeseasonalize <- ts(deseasonalize, start = c(1,1), frequency = 63)
plot1 <- autoplot(tsdeseasonalize, col = "black")+  
  theme_light()
plot3 <- ggAcf(deseasonalize, lag.max=400,  col = "black") + theme_light()

grid.arrange(plot1, plot3)

trend <- ts_train_dec$trend
random <- ts_train_dec$random
detrend<-deseasonalize-trend

tsdetrend <- ts(detrend, start = c(1,1), frequency = 63)
plot2 <- autoplot(tsdetrend, col = "black")+  
  theme_light()

plot4 <- ggAcf(detrend, lag.max=400,  col = "black") +  theme_light()
grid.arrange(plot2, plot4)


```

For Arima model, (p,d,q) values should be chosen. For this purpose, it can be looked at the ACF and PACF plots. Looking at the ACF, for 'q' value 1 can be chosen and looking at the PACF, for 'p' value 1 or 4 can be chosen as well.

```{r choose, echo=FALSE, message=FALSE, warning=FALSE}
plot5 <- ggAcf(detrend, lag.max=15,  col = "maroon1") +  
  theme_light()
plot6 <- ggAcf(detrend, lag.max=15,  col = "maroon1", type=c("partial")) +  
  theme_light()
grid.arrange(plot5, plot6)
```

The AIC and BIC values of models that are suggested can be seen below. Also, auto.arima function is used. Smaller AIC and BIC values means the model is better. So, looking at AIC and BIC values, (1,0,0) model that auto.arima is suggested is best among them. We can proceed with this model.

```{r try1, echo=FALSE, message=FALSE, warning=FALSE}
library(stats)

model1 <- arima(detrend, order=c(1,0,1))
print(model1)
AIC(model1)
BIC(model1)
model2 <- arima(detrend, order=c(4,0,1))
print(model2)
AIC(model2)
BIC(model2)

final=auto.arima(detrend, seasonal=FALSE, trace=T)
print(final)
AIC(final)
BIC(final)

```

Below, there is comparison with training set and fitted values of final model to understand how the final model is learned data. There is plot of random series, plot of logarithm of actual series and plot of actual series. The brown lines belong to fitted model and light blue lines belong to actual series.

```{r plot1, echo=FALSE, message=FALSE, warning=FALSE}
fitted <- random - residuals(final)
fitted_actual_log <- fitted+trend+seasonal
fitted_actual <- exp(fitted_actual_log)
ts_sold<-ts(train$sold_count,freq=63, start=c(1,1))

both <- cbind(random, fitted)

plot(random, main="Plot of Random Series", col="cyan2")
points(fitted, type = "l", col = "brown2")

plot(ts_train, main="Plot of Log of Actual Series",col="cyan2")
points(fitted_actual_log, type = "l", col = "brown2")

plot(ts_sold, main="Plot of Actual Series",col="cyan2")
points(fitted_actual, type = "l", col = "brown2")

```

The model can be improved with adding regressors. For deciding which regressor that should be added, it should be looked the correlation matrix that contains different attributes. Looking at the correlations with logarithm of sold counts, category_favored and basket_count attributes should be chosen.

```{r choose regressors1, echo=FALSE, message=FALSE, warning=FALSE}

data <- train[,-c(2,3,4)]
#view(data)
ggcorrplot(corr = cor(data),hc.order = TRUE, type = "upper",lab = TRUE, title = "Correlation Matrix", colors = c("lightgoldenrod1","lightgoldenrod3","lightgoldenrod4"), show.legend = FALSE, lab_size = 3)


```

The regressors that are chosen are added to the model. It is clear that the new model's AIC and BIC values are much lower, therefore the model is better and it can be proceed with this model. Also, the fitted values are better for training set and it can be seen in the plots. As you can see, these fitted plots are better than the model that previously chosen.

```{r model with regressor1, echo=FALSE, message=FALSE, warning=FALSE}
xreg <- cbind(train$basket_count, train$category_favored)
model3 <- arima(detrend, order=c(1,0,0), xreg= xreg)
print(model3)
AIC(model3)
BIC(model3)

```

```{r plot model1, echo=FALSE, message=FALSE, warning=FALSE}
fitted <- random - residuals(model2)
fitted_actual_log <- fitted+trend+seasonal
fitted_actual <- exp(fitted_actual_log)
ts_sold<-ts(train$sold_count,freq=63, start=c(1,1))

both <- cbind(random, fitted)

plot(random, main="Plot of Random Series", col="cyan2")
points(fitted, type = "l", col = "brown2")

plot(ts_train, main="Plot of Log of Actual Series",col="cyan2")
points(fitted_actual_log, type = "l", col = "brown2")

plot(ts_sold, main="Plot of Actual Series",col="cyan2")
points(fitted_actual, type = "l", col = "brown2")

```

The predictions will be made with final model. The predicted values with test set can be seen below. The mean absolute error for each day is plotted and the weighted mean absolute percentage error for this prediction can be seen as well.

```{r prediction1, echo=FALSE, message=FALSE, warning=FALSE}
pred_bc<- predict(train$basket_count)
new_basket_count <- pred_bc$fitted[1:7]
pred_cf<- predict(train$category_favored)
new_category_favored <- pred_cf$fitted[1:7]
newxreg <- cbind(new_basket_count, new_category_favored)
predicted <- predict(model3, newxreg=newxreg, h=7)$pred
predicted <- ts(predicted,frequency = 7,start=c(1,1))


last_trend <-tail(trend[!is.na(trend)],1)
seasonality <- seasonal[6:12]
predicted_actual_log <- predicted+last_trend+seasonality
predicted_actual <- exp(predicted_actual_log)

```


```{r plotfinal1, echo=FALSE, message=FALSE, warning=FALSE}
sevendays <- as.Date(as.Date("2021-06-24"):as.Date("2021-06-30"))

myvalues <- xts(x = data.frame(predicted_actual, test$sold_count), order.by = sevendays, frequency = 7)
colnames(myvalues) <- c("Prediction", "Real Values")
plot(myvalues, main = "Predictions and Real Values", legend.loc = "bottomleft")

```


```{r final1, echo=FALSE, message=FALSE, warning=FALSE}
mae <- data.frame( (abs(myvalues$Prediction- test$sold_count)/test$sold_count), sevendays)
names(mae)<- c("MeanAbsoluteErrors", "Date")
plotmae <- ggplot(mae , aes(x= Date , y=MeanAbsoluteErrors)) 

plotmae +  geom_line(color="brown2") +  theme_light()+ labs(title= "Mean Absolute Errors",x= "Days")
weighted_mape <- sum((mae$MeanAbsoluteErrors*100*test$sold_count)/ sum(test$sold_count))

print(paste(" Weighted Mean Absolute Percentage Error : ",weighted_mape))


```

# PRODUCT 2 - Sleepy Baby Wipes

```{r product2, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE}
product2 <- products[[2]]
setDT(product2)

product2$price[product2$price == 0] <- mean(product2$price[product2$price !=0])
product2$basket_count[product2$basket_count == 0] <- mean(product2$basket_count[product2$basket_count !=0])
product2$favored_count[product2$favored_count == 0] <- mean(product2$favored_count[product2$favored_count !=0])
product2$sold_count[product2$sold_count == 0] <- mean(product2$sold_count[product2$sold_count !=0])
product2$visit_count[product2$visit_count == 0] <- mean(product2$visit_count[product2$visit_count !=0])
product2$category_favored[product2$category_favored == 0] <- mean(product2$category_favored[product2$category_favored !=0])
product2$category_sold[product2$category_sold == 0] <- mean(product2$category_sold[product2$category_sold !=0])
product2$category_visits[product2$category_visits == 0] <- mean(product2$category_visits[product2$category_visits !=0])
product2$category_basket[product2$category_basket == 0] <- mean(product2$category_basket[product2$category_basket !=0])
product2$category_brand_sold[product2$category_brand_sold == 0] <- mean(product2$category_brand_sold[product2$category_brand_sold !=0])
product2$ty_visits[product2$ty_visits == 0] <- mean(product2$ty_visits[product2$ty_visits !=0])

train2 <- product2[product2$event_date <= as.Date("2021-06-23")]
test2 <- product2[product2$event_date > as.Date("2021-06-23")& product2$event_date <= as.Date("2021-06-30")]
#view(train)

```

It should be looked at the plot of data and examined the seasonalities and trend at first. It's clear that the variance of data is very large, from now on, it can be continued with the logarithm of sold count. Below, you can see the actual plot and the plot with the values that are taken logarithms. There is a slightly increasing trend, especially in the end of the plot. There can't be seen any significant seasonality. To look further, there is a plot of 3 months of 2021 - March, April and May -. Again, the seasonality isn't significant, though it can be said there is a spike in the plot at the beginning of the month. In May, there is a big rising probably due to Covid-19 conditions. In conclusion, it can be said that there is monthly seasonality but it isn't very clear.

```{r plot daily2, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE}

sold_log <- log(train2$sold_count)
train2 <- cbind(train2, sold_log)

ggplot(train2,aes(x=event_date,y=sold_count)) + geom_line(color="brown2") +  theme_light()+labs(x = "Date",
       y = "Sales Quantity of the Product")

ggplot(train2,aes(x=event_date,y=sold_log)) + geom_line(color="brown2") +  theme_light()+labs(x = "Date",
       y = "Log of Sales Quantity of the Product") + geom_smooth(color="black", linetype="longdash", se = FALSE)

ggplot(train2[event_date<='2021-05-30' & event_date>='2021-03-01'] ,aes(x=event_date,y=sold_log)) + geom_line(color="darkolivegreen4") +  theme_light()+labs(x = "3 months of 2021",
       y = "Log of Sales Quantity of the Product")

```

Before decomposing, to make better decision, it can be looked to the autocorrelation plot of data. Below, it can be seen that there is a spike at lag 27. We can determine the frequency as 27 just because the seasonality wasn't very significant.

```{r plot acf2, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE}

plotacfp2 <- ggAcf(train2$sold_log, lag.max=80,  col = "coral4") + theme_light()
grid.arrange(plotacfp2)


```

Now, the data will be decomposed in order to make random series."Additive" type of decomposing will be used because the variance of data is very low due to switching to logarithm. Plots of deseasonalized and the random series their autocorrelations can be seen below.


```{r decomposition2, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE}

ts_train2<-ts(train2$sold_log,freq=27, start=c(1,1))
ts_train_dec2<-decompose(ts_train2,type="additive")
plot(ts_train_dec2)

seasonal2<- ts_train_dec2$seasonal
deseasonalize2<-ts_train2-seasonal2

tsdeseasonalize2 <- ts(deseasonalize2, start = c(1,1), frequency = 27)
plot1p2 <- autoplot(tsdeseasonalize2, col = "black")+  
  theme_light()
plot3p2 <- ggAcf(deseasonalize2, lag.max=400,  col = "black") + theme_light()

grid.arrange(plot1p2, plot3p2)

trend2 <- ts_train_dec2$trend
random2 <- ts_train_dec2$random
detrend2 <-deseasonalize2-trend2

tsdetrend2 <- ts(detrend2, start = c(1,1), frequency = 27)
plot2p2 <- autoplot(tsdetrend2, col = "black")+  
  theme_light()

plot4p2 <- ggAcf(detrend2, lag.max=400,  col = "black") +  theme_light()
grid.arrange(plot2p2, plot4p2)


```

For Arima model, (p,d,q) values should be chosen. For this purpose, it can be looked at the ACF and PACF plots. Looking at the ACF, for 'q' value 2 or 10 can be chosen and looking at the PACF, for 'p' value 2 can be chosen.

```{r choose2, echo=FALSE, message=FALSE, warning=FALSE}
plot5p2 <- ggAcf(detrend2, lag.max=15,  col = "maroon1") +  
  theme_light()
plot6p2 <- ggAcf(detrend2, lag.max=15,  col = "maroon1", type=c("partial")) +  
  theme_light()
grid.arrange(plot5p2, plot6p2)
```

The AIC and BIC values of models that are suggested can be seen below. Also, auto.arima function is used. So, looking at AIC and BIC values, (2,0,2) model that is suggested above is best among them. We can proceed with this model.

```{r try2, echo=FALSE, message=FALSE, warning=FALSE}
library(stats)

model1p2 <- arima(detrend2, order=c(2,0,2))
print(model1p2)
AIC(model1p2)
BIC(model1p2)
model2p2 <- arima(detrend2, order=c(2,0,10))
print(model2p2)
AIC(model2p2)
BIC(model2p2)

finalp2=auto.arima(detrend2, seasonal=FALSE, trace=T)
print(finalp2)
AIC(finalp2)
BIC(finalp2)

```

Below, there is comparison with training set and fitted values of final model to understand how the final model is learned data. There is plot of random series, plot of logarithm of actual series and plot of actual series. The brown lines belong to fitted model and light blue lines belong to actual series.

```{r plot2, echo=FALSE, message=FALSE, warning=FALSE}
fitted2 <- random2 - residuals(model1p2)
fitted_actual_log2 <- fitted2 + trend2 + seasonal2
fitted_actual2 <- exp(fitted_actual_log2)
ts_sold2 <-ts(train2$sold_count,freq=27, start=c(1,1))

both <- cbind(random2, fitted2)

plot(random2, main="Plot of Random Series", col="cyan2")
points(fitted2, type = "l", col = "brown2")

plot(ts_train2, main="Plot of Log of Actual Series",col="cyan2")
points(fitted_actual_log2, type = "l", col = "brown2")

plot(ts_sold2, main="Plot of Actual Series",col="cyan2")
points(fitted_actual2, type = "l", col = "brown2")

```

The model can be improved with adding regressors. For deciding which regressor that should be added, it should be looked the correlation matrix that contains different attributes. Looking at the correlations with logarithm of sold counts, category_sold and basket_count attributes should be chosen.

```{r choose regressors2, echo=FALSE, message=FALSE, warning=FALSE}

data2 <- train2[,-c(2,3,4)]
#view(data)
ggcorrplot(corr = cor(data2),hc.order = TRUE, type = "upper",lab = TRUE, title = "Correlation Matrix", colors = c("lightgoldenrod1","lightgoldenrod3","lightgoldenrod4"), show.legend = FALSE, lab_size = 3)


```

The regressors that are chosen are added to the model. It is clear that the new model's AIC and BIC values are much lower, therefore the model is better and it can be proceed with this model. Also, the fitted values are better for training set and it can be seen in the plots. As you can see, these fitted plots are better than the model that previously chosen.

```{r model with regressor2, echo=FALSE, message=FALSE, warning=FALSE}
xreg2 <- cbind(train2$basket_count, train2$category_sold)
model3p2 <- arima(detrend2, order=c(2,0,2), xreg= xreg2)
print(model3p2)
AIC(model3p2)
BIC(model3p2)

```


```{r plot model3p2, echo=FALSE, message=FALSE, warning=FALSE}
fitted2 <- random2 - residuals(model3p2)
fitted_actual_log2 <- fitted2+trend2+seasonal2
fitted_actual2 <- exp(fitted_actual_log2)
ts_sold2<-ts(train2$sold_count,freq=27, start=c(1,1))

both <- cbind(random2, fitted2)

plot(random2, main="Plot of Random Series", col="cyan2")
points(fitted2, type = "l", col = "brown2")

plot(ts_train2, main="Plot of Log of Actual Series",col="cyan2")
points(fitted_actual_log2, type = "l", col = "brown2")

plot(ts_sold2, main="Plot of Actual Series",col="cyan2")
points(fitted_actual2, type = "l", col = "brown2")

```

The predictions will be made with final model. The predicted values with test set can be seen below. The mean absolute error for each day is plotted and the weighted mean absolute percentage error for this prediction can be seen as well.

```{r prediction2, echo=FALSE, message=FALSE, warning=FALSE}
pred_bc<-predict(train2$basket_count)
pred_cs<-predict(train$category_sold)
new_basket_count <- pred_bc$fitted[1:7]
new_category_sold <- pred_cs$fitted[1:7]
new_xreg <- cbind(new_basket_count, new_category_sold)
predicted <- predict(model3p2, newxreg = new_xreg, h=7)$pred
predicted <- ts(predicted,frequency = 7,start=c(1,1))


last_trend <-tail(trend2[!is.na(trend2)],1)
seasonality <- seasonal2[18:24]
predicted_actual_log <- predicted+last_trend+seasonality
predicted_actual <- exp(predicted_actual_log)

```


```{r plotfinal2, echo=FALSE, message=FALSE, warning=FALSE}
sevendays <- as.Date(as.Date("2021-06-24"):as.Date("2021-06-30"))

myvalues <- xts(x = data.frame(predicted_actual, test2$sold_count), order.by = sevendays, frequency = 7)
colnames(myvalues) <- c("Prediction", "Real Values")
plot(myvalues, main = "Predictions and Real Values", legend.loc = "bottomleft")

```


```{r final2, echo=FALSE, message=FALSE, warning=FALSE}
mae <- data.frame( (abs(myvalues$Prediction- test2$sold_count)/test2$sold_count), sevendays)
names(mae)<- c("MeanAbsoluteErrors", "Date")
plotmae <- ggplot(mae , aes(x= Date , y=MeanAbsoluteErrors)) 

plotmae +  geom_line(color="brown2") +  theme_light()+ labs(title= "Mean Absolute Errors",x= "Days")
weighted_mape <- sum((mae$MeanAbsoluteErrors*100*test2$sold_count)/ sum(test2$sold_count))

print(paste(" Weighted Mean Absolute Percentage Error : ",weighted_mape))


```



```{r Get Data Product 3, include=FALSE}

product3 <- products[[3]]
setDT(product3)

product3$price[product3$price == 0] <- mean(product3$price[product3$price !=0])
product3$basket_count[product3$basket_count == 0] <- mean(product3$basket_count[product3$basket_count !=0])
product3$favored_count[product3$favored_count == 0] <- mean(product3$favored_count[product3$favored_count !=0])
product3$sold_count[product3$sold_count == 0] <- mean(product3$sold_count[product3$sold_count !=0])
product3$visit_count[product3$visit_count == 0] <- mean(product3$visit_count[product3$visit_count !=0])
product3$category_favored[product3$category_favored == 0] <- mean(product3$category_favored[product3$category_favored !=0])
product3$category_sold[product3$category_sold == 0] <- mean(product3$category_sold[product3$category_sold !=0])
product3$category_visits[product3$category_visits == 0] <- mean(product3$category_visits[product3$category_visits !=0])
product3$category_basket[product3$category_basket == 0] <- mean(product3$category_basket[product3$category_basket !=0])
product3$category_brand_sold[product3$category_brand_sold == 0] <- mean(product3$category_brand_sold[product3$category_brand_sold !=0])
product3$ty_visits[product3$ty_visits == 0] <- mean(product3$ty_visits[product3$ty_visits !=0])

train <- product3[product3$event_date <= as.Date("2021-06-23")]
test <- product3[product3$event_date > as.Date("2021-06-23")&product3$event_date <= as.Date("2021-06-30")]
#view(train)


```

# PRODUCT 3 - Xiaomi Bluetooth Headphones

First of all, it's clear that the variance of data is very large, from now on, it can be continued with the logarithm of sold count. Below, you can see the actual plot and the plot with the values that are taken logarithms. At below,looking at the plots of the product; in line graph it can be observed that the sales have variance, in some dates the plot has peaks and also there might be a cyclical behaviour which is an indicator for seasonality. 


In order to proceed further,the data should be decomposed,a frequency value should be chosen. 30 and 7 day frequency can be selected and the data can be decomposed accordingly. Along with 30 and 7 day frequency, ACF plot of the data can be examined and in the lag that we see high autocorrelation it can be chosen as another trial frequency to decompose. Since variance don't seem to be increasing, additive type of decomposition can be used for decomposition. Below, the random series can be seen.

```{r plot daily product 3, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE}

sold_log <- log(train$sold_count)
train <- cbind(train, sold_log)

ggplot(train,aes(x=event_date,y=sold_count)) + geom_line(color="brown2") +  theme_light()+labs(x = "Date",
       y = "Sales Quantity of the Product 3")

ggplot(train,aes(x=event_date,y=sold_log)) + geom_line(color="brown2") +  theme_light()+labs(x = "Date",
       y = "Log of Sales Quantity of the Product 3") + geom_smooth(color="black", linetype="longdash", se = FALSE)

ggplot(train[event_date<='2021-05-30' & event_date>='2021-03-01'] ,aes(x=event_date,y=sold_log)) + geom_line(color="darkolivegreen4") +  theme_light()+labs(x = "First 3 months of 2021",
       y = "Log of Sales Quantity of the Product 3")

```


```{r decomposition weekly product 3, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE}

ts_train<-ts(train$sold_log,freq=7, start=c(1,1))
ts_train_dec<-decompose(ts_train,type="additive")
plot(ts_train_dec)

seasonal<- ts_train_dec$seasonal
deseasonalize<-ts_train-seasonal

tsdeseasonalize <- ts(deseasonalize, start = c(1,1), frequency = 7)
plot1 <- autoplot(tsdeseasonalize, col = "black")+  
  theme_light()
plot3 <- ggAcf(deseasonalize, lag.max=400,  col = "black") + theme_light()

grid.arrange(plot1, plot3)

trend <- ts_train_dec$trend
random <- ts_train_dec$random
detrend<-deseasonalize-trend

tsdetrend <- ts(detrend, start = c(1,1), frequency = 7)
plot2 <- autoplot(tsdetrend, col = "black")+  
  theme_light()

plot4 <- ggAcf(detrend, lag.max=400,  col = "black") +  theme_light()
grid.arrange(plot2, plot4)


```

```{r decomposition monthly product 3, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE}

ts_train<-ts(train$sold_log,freq=30, start=c(1,1))
ts_train_dec<-decompose(ts_train,type="additive")
plot(ts_train_dec)

seasonal<- ts_train_dec$seasonal
deseasonalize<-ts_train-seasonal

tsdeseasonalize <- ts(deseasonalize, start = c(1,1), frequency = 30)
plot1 <- autoplot(tsdeseasonalize, col = "black")+  
  theme_light()
plot3 <- ggAcf(deseasonalize, lag.max=400,  col = "black") + theme_light()

grid.arrange(plot1, plot3)

trend <- ts_train_dec$trend
random <- ts_train_dec$random
detrend<-deseasonalize-trend

tsdetrend <- ts(detrend, start = c(1,1), frequency = 30)
plot2 <- autoplot(tsdetrend, col = "black")+  
  theme_light()

plot4 <- ggAcf(detrend, lag.max=400,  col = "black") +  theme_light()
grid.arrange(plot2, plot4)


```

The above decomposition series belong to time series with 7 and 30 days frequency, respectively.

```{r plot acf product 3, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE}

plotacf <- ggAcf(train$sold_log, lag.max=80,  col = "coral4") + theme_light()
grid.arrange(plotacf)

```

Looking at the ACF plot of the series, highest ACF value belongs to lag 32, so time series decomposition with 32 day frequency would be sufficient.

```{r decomposition product 3, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE}

ts_train<-ts(train$sold_log,freq=32, start=c(1,1))
ts_train_dec<-decompose(ts_train,type="additive")
plot(ts_train_dec)

seasonal<- ts_train_dec$seasonal
deseasonalize<-ts_train-seasonal

tsdeseasonalize <- ts(deseasonalize, start = c(1,1), frequency = 32)
plot1 <- autoplot(tsdeseasonalize, col = "black")+  
  theme_light()
plot3 <- ggAcf(deseasonalize, lag.max=400,  col = "black") + theme_light()

grid.arrange(plot1, plot3)

trend <- ts_train_dec$trend
random <- ts_train_dec$random
detrend<-deseasonalize-trend

tsdetrend <- ts(detrend, start = c(1,1), frequency = 32)
plot2 <- autoplot(tsdetrend, col = "black")+  
  theme_light()

plot4 <- ggAcf(detrend, lag.max=400,  col = "black") +  theme_light()
grid.arrange(plot2, plot4)


```
In time series decomposition, it is assumed that the random part is randomly distributed with mean zero and standard deviation 1; in order to decide on the best frequency, the random part of the decomposed series should be observed. In this case, the random part of the decomposed time series with 7 day frequency seem to be closer to randomly distributed series with mean zero and std dev 1, so it is chosen as the final decomposition.

```{r decomposition weekly product 3 final, fig.align='center', include=FALSE,echo=FALSE, message=FALSE, warning=FALSE}

ts_train<-ts(train$sold_log,freq=7, start=c(1,1))
ts_train_dec<-decompose(ts_train,type="additive")
plot(ts_train_dec)

seasonal<- ts_train_dec$seasonal
deseasonalize<-ts_train-seasonal

tsdeseasonalize <- ts(deseasonalize, start = c(1,1), frequency = 7)
plot1 <- autoplot(tsdeseasonalize, col = "black")+  
  theme_light()
plot3 <- ggAcf(deseasonalize, lag.max=400,  col = "black") + theme_light()

grid.arrange(plot1, plot3)

trend <- ts_train_dec$trend
random <- ts_train_dec$random
detrend<-deseasonalize-trend

tsdetrend <- ts(detrend, start = c(1,1), frequency = 7)
plot2 <- autoplot(tsdetrend, col = "black")+  
  theme_light()

plot4 <- ggAcf(detrend, lag.max=400,  col = "black") +  theme_light()
grid.arrange(plot2, plot4)


```
After the decomposition, (p,d,q) values should be chosen for the model. For this task, ACF and PACF will be examined.For q, peaks at ACF function can be chosen and for p values, peaks at PACF function can be chosen. 
Looking at the ACF, for 'q' value 3 or 4 may be selected and looking at the PACF, for 'p' value 3  may be selected. Also, auto.arima function is used as well. The AIC and BIC values of models that are suggested can be seen below.  Smaller AIC and BIC values means the model is better. So, looking at AIC and BIC values, (3,0,3) model that auto.arima has suggested is best among them.


```{r choose product 3, echo=FALSE, message=FALSE, warning=FALSE}
plot5 <- ggAcf(detrend, lag.max=15,  col = "maroon1") +  
  theme_light()
plot6 <- ggAcf(detrend, lag.max=15,  col = "maroon1", type=c("partial")) +  
  theme_light()
grid.arrange(plot5, plot6)
```


```{r try product 3, echo=FALSE, message=FALSE, warning=FALSE}
library(stats)

model1 <- arima(detrend, order=c(1,0,1))
print(model1)
AIC(model1)
BIC(model1)
model2 <- arima(detrend, order=c(3,0,3))
print(model2)
AIC(model2)
BIC(model2)

final=auto.arima(detrend, seasonal=FALSE, trace=T)
print(final)
AIC(final)
BIC(final)

final <- arima(detrend, order=c(3,0,3))


```

Below, there is comparison with training set and fitted values of final model to understand how the final model has learned data. There is plot of random series, plot of logarithm of actual series and plot of actual series. The brown lines belong to fitted model and light blue lines belong to actual series.

```{r plot product 3, echo=FALSE, message=FALSE, warning=FALSE}
fitted <- random - residuals(final)
fitted_actual_log <- fitted+trend+seasonal
fitted_actual <- exp(fitted_actual_log)
ts_sold<-ts(train$sold_count,freq=7, start=c(1,1))

both <- cbind(random, fitted)

plot(random, main="Plot of Random Series", col="cyan2")
points(fitted, type = "l", col = "brown2")

plot(ts_train, main="Plot of Log of Actual Series",col="cyan2")
points(fitted_actual_log, type = "l", col = "brown2")

plot(ts_sold, main="Plot of Actual Series",col="cyan2")
points(fitted_actual, type = "l", col = "brown2")

```
The fitted values have captured partly behaviour of the series, however prediction values seem to be overpredicting in peak values of the original data. The model can be improved with adding regressors. For deciding which regressor that should be added, the correlation matrix that contains different attributes should be examined. Looking at the correlations with logarithm of category_sold and visit_count attributes should be chosen.

```{r choose regressorsproduct 3, echo=FALSE, message=FALSE, warning=FALSE}

data <- train[,-c(2,3,4)]
#view(data)
ggcorrplot(corr = cor(data),hc.order = TRUE, type = "upper",lab = TRUE, title = "Correlation Matrix", colors = c("lightgoldenrod1","lightgoldenrod3","lightgoldenrod4"), show.legend = FALSE)


```


```{r model with regressor 3-2, echo=FALSE, message=FALSE, warning=FALSE}
xreg <- cbind(train$category_sold, train$visit_count)
model3 <- arima(detrend, order=c(3,0,3), xreg= xreg)
print(model3)
AIC(model3)
BIC(model3)

```
The regressors that are chosen are added to the model. It is clear that the new model's AIC and BIC values are much lower, therefore the model is better and it can be proceeded with this model.

```{r plot model3, echo=FALSE, message=FALSE, warning=FALSE}
fitted <- random - residuals(model3)
fitted_actual_log <- fitted+trend+seasonal
fitted_actual <- exp(fitted_actual_log)
ts_sold<-ts(train$sold_count,freq=7, start=c(1,1))

both <- cbind(random, fitted)

plot(random, main="Plot of Random Series", col="cyan2")
points(fitted, type = "l", col = "brown2")

plot(ts_train, main="Plot of Log of Actual Series",col="cyan2")
points(fitted_actual_log, type = "l", col = "brown2")

plot(ts_sold, main="Plot of Actual Series",col="cyan2")
points(fitted_actual, type = "l", col = "brown2")

```
The predictions will be made with final model. The predicted values with test set can be seen below. The mean absolute error for each day is plotted and the weighted mean absolute percentage error for this prediction can be seen as well.


```{r prediction3, echo=FALSE, message=FALSE, warning=FALSE}
pred_bc<-predict(train$category_sold)
pred_cb<-predict(train$visit_count)
new_category_sold <- pred_bc$fitted[1:7]
new_visit_count <- pred_cb$fitted[1:7]
new_xreg <- cbind(new_category_sold, new_visit_count)
predicted <- predict(model3, newxreg = new_xreg, h=7)$pred
predicted <- ts(predicted,frequency = 7,start=c(1,1))


last_trend <-tail(trend[!is.na(trend)],1)
seasonality <- seasonal[18:24]
predicted_actual_log <- predicted+last_trend+seasonality
predicted_actual <- exp(predicted_actual_log)
(predicted_actual)

```


```{r plotfinal3, echo=FALSE, message=FALSE, warning=FALSE}
sevendays <- as.Date(as.Date("2021-06-24"):as.Date("2021-06-30"))

myvalues <- xts(x = data.frame(predicted_actual, test$sold_count), order.by = sevendays, frequency = 7)
colnames(myvalues) <- c("Prediction", "Real Values")
plot(myvalues, main = "Predictions and Real Values", legend.loc = "bottomleft")

```


```{r final3, echo=FALSE, message=FALSE, warning=FALSE}
mae <- data.frame( (abs(myvalues$Prediction- test$sold_count)/test$sold_count), sevendays)
names(mae)<- c("MeanAbsoluteErrors", "Date")
plotmae <- ggplot(mae , aes(x= Date , y=MeanAbsoluteErrors)) 

plotmae +  geom_line(color="brown2") +  theme_light()+ labs(title= "Mean Absolute Errors",x= "Days")
weighted_mape <- sum((mae$MeanAbsoluteErrors*100*test$sold_count)/ sum(test$sold_count))

print(paste(" Weighted Mean Absolute Percentage Error : ",weighted_mape))


```
Looking at the plot for Predictions vs Actual Sales, it can be said that the model captured the behaviour of the data but there is almost a constant difference between prediction and actual sales, that is probably why the WMAPE is high, in order to solve this issue, further investigation can be made.


# Product 4 - Fakir Vacuum Cleaner


```{r Get Data Product 4, include=FALSE,echo=FALSE}

product4 <- products[[4]]
setDT(product4)

product4$price[product4$price == 0] <- mean(product4$price[product4$price !=0])
product4$basket_count[product4$basket_count == 0] <- mean(product4$basket_count[product4$basket_count !=0])
product4$favored_count[product4$favored_count == 0] <- mean(product4$favored_count[product4$favored_count !=0])
product4$sold_count[product4$sold_count == 0] <- mean(product4$sold_count[product4$sold_count !=0])
product4$visit_count[product4$visit_count == 0] <- mean(product4$visit_count[product4$visit_count !=0])
product4$category_favored[product4$category_favored == 0] <- mean(product4$category_favored[product4$category_favored !=0])
product4$category_sold[product4$category_sold == 0] <- mean(product4$category_sold[product4$category_sold !=0])
product4$category_visits[product4$category_visits == 0] <- mean(product4$category_visits[product4$category_visits !=0])
product4$category_basket[product4$category_basket == 0] <- mean(product4$category_basket[product4$category_basket !=0])
product4$category_brand_sold[product4$category_brand_sold == 0] <- mean(product4$category_brand_sold[product4$category_brand_sold !=0])
product4$ty_visits[product4$ty_visits == 0] <- mean(product4$ty_visits[product4$ty_visits !=0])

train <- product4[product4$event_date <= as.Date("2021-06-23")]
test <- product4[product4$event_date > as.Date("2021-06-23")&product4$event_date <= as.Date("2021-06-30")]


```
First of all, it's clear that the variance of data is very large, from now on, it can be continued with the logarithm of sold count. Below, you can see the actual plot and the plot with the values that are taken logarithms. 
Below, you can see the actual plot and the plot with the values that are taken logarithms. There is no significant trend. There may  be seasonality, to look further, there is a plot of 3 months of 2021 - March, April and May. The seasonality isn't easily observed, though it can be said there is a spike in the plot at the end of the month. In May, there is a big rising probably due to Covid-19 conditions. In conclusion, it can be said that there is monthly seasonality but it isn't very clear.

```{r plot daily product 4, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE}

sold_log <- log(train$sold_count)
train <- cbind(train, sold_log)

ggplot(train,aes(x=event_date,y=sold_count)) + geom_line(color="brown2") +  theme_light()+labs(x = "Date",
       y = "Sales Quantity of the Product 3")

ggplot(train,aes(x=event_date,y=sold_log)) + geom_line(color="brown2") +  theme_light()+labs(x = "Date",
       y = "Log of Sales Quantity of the Product 3") + geom_smooth(color="black", linetype="longdash", se = FALSE)

ggplot(train[event_date<='2021-05-30' & event_date>='2021-03-01'] ,aes(x=event_date,y=sold_log)) + geom_line(color="darkolivegreen4") +  theme_light()+labs(x = "First 3 months of 2021",
       y = "Log of Sales Quantity of the Product 3")

```
30 and 7 day frequency can be selected and the data can be decomposed accordingly. Along with 30 and 7 day frequency, ACF plot of the data can be examined and in the lag that we see high autocorrelation it can be chosen as another trial frequency to decompose. Since variance don't seem to be increasing, additive type of decomposition can be used for decomposition. Below, the random series can be seen.
```{r decomposition weekly product 4, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE}

ts_train<-ts(train$sold_log,freq=7, start=c(1,1))
ts_train_dec<-decompose(ts_train,type="additive")
plot(ts_train_dec)

seasonal<- ts_train_dec$seasonal
deseasonalize<-ts_train-seasonal

tsdeseasonalize <- ts(deseasonalize, start = c(1,1), frequency = 7)
plot1 <- autoplot(tsdeseasonalize, col = "black")+  
  theme_light()
plot3 <- ggAcf(deseasonalize, lag.max=400,  col = "black") + theme_light()

grid.arrange(plot1, plot3)

trend <- ts_train_dec$trend
random <- ts_train_dec$random
detrend<-deseasonalize-trend

tsdetrend <- ts(detrend, start = c(1,1), frequency = 7)
plot2 <- autoplot(tsdetrend, col = "black")+  
  theme_light()

plot4 <- ggAcf(detrend, lag.max=400,  col = "black") +  theme_light()
grid.arrange(plot2, plot4)


```

```{r decomposition monthly product 4, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE}

ts_train<-ts(train$sold_log,freq=30, start=c(1,1))
ts_train_dec<-decompose(ts_train,type="additive")
plot(ts_train_dec)

seasonal<- ts_train_dec$seasonal
deseasonalize<-ts_train-seasonal

tsdeseasonalize <- ts(deseasonalize, start = c(1,1), frequency = 30)
plot1 <- autoplot(tsdeseasonalize, col = "black")+  
  theme_light()
plot3 <- ggAcf(deseasonalize, lag.max=400,  col = "black") + theme_light()

grid.arrange(plot1, plot3)

trend <- ts_train_dec$trend
random <- ts_train_dec$random
detrend<-deseasonalize-trend

tsdetrend <- ts(detrend, start = c(1,1), frequency = 30)
plot2 <- autoplot(tsdetrend, col = "black")+  
  theme_light()

plot4 <- ggAcf(detrend, lag.max=400,  col = "black") +  theme_light()
grid.arrange(plot2, plot4)


```
The above decomposition series belong to time series with 7 and 30 days frequency, respectively.

```{r plot acf product 4, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE}

plotacf <- ggAcf(train$sold_log, lag.max=80,  col = "coral4") + theme_light()
grid.arrange(plotacf)

```

Looking at the ACF plot of the series, highest ACF value belongs to lag 35, so time series decomposition with 35 day frequency would be sufficient.
```{r decomposition product 4, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE}

ts_train<-ts(train$sold_log,freq=35, start=c(1,1))
ts_train_dec<-decompose(ts_train,type="additive")
plot(ts_train_dec)

seasonal<- ts_train_dec$seasonal
deseasonalize<-ts_train-seasonal

tsdeseasonalize <- ts(deseasonalize, start = c(1,1), frequency = 35)
plot1 <- autoplot(tsdeseasonalize, col = "black")+  
  theme_light()
plot3 <- ggAcf(deseasonalize, lag.max=400,  col = "black") + theme_light()

grid.arrange(plot1, plot3)

trend <- ts_train_dec$trend
random <- ts_train_dec$random
detrend<-deseasonalize-trend

tsdetrend <- ts(detrend, start = c(1,1), frequency = 35)
plot2 <- autoplot(tsdetrend, col = "black")+  
  theme_light()

plot4 <- ggAcf(detrend, lag.max=400,  col = "black") +  theme_light()
grid.arrange(plot2, plot4)


```

In this case, the random part of the decomposed time series with 35 day frequency seem to be closer to randomly distributed series with mean zero and std dev 1, so it is chosen as the final decomposition.

```{r choose product 4, echo=FALSE, message=FALSE, warning=FALSE}
plot5 <- ggAcf(detrend, lag.max=15,  col = "maroon1") +  
  theme_light()
plot6 <- ggAcf(detrend, lag.max=15,  col = "maroon1", type=c("partial")) +  
  theme_light()
grid.arrange(plot5, plot6)
```

Looking at the ACF, for 'q' value 1 or 3 may be selected and looking at the PACF, for 'p' value 1  may be selected. Also, auto.arima function is used as well. The AIC and BIC values of models that are suggested can be seen below. Looking at AIC and BIC values, (1,0,3) model which has been built with respect to observations in ACF and PACF plots is best among them. Also it is the same model that auto arima has suggested.



```{r try product 4, echo=FALSE, message=FALSE, warning=FALSE}
library(stats)

model1 <- arima(detrend, order=c(1,0,1))
print(model1)
AIC(model1)
BIC(model1)
model2 <- arima(detrend, order=c(1,0,3))
print(model2)
AIC(model2)
BIC(model2)

final=auto.arima(detrend, seasonal=FALSE, trace=T)
print(final)
AIC(final)
BIC(final)

```
Below, there is comparison with training set and fitted values of final model to understand how the final model has learned data. There is plot of random series, plot of logarithm of actual series and plot of actual series. The brown lines belong to fitted model and light blue lines belong to actual series.


```{r plot product 4, echo=FALSE, message=FALSE, warning=FALSE}
fitted <- random - residuals(final)
fitted_actual_log <- fitted+trend+seasonal
fitted_actual <- exp(fitted_actual_log)
ts_sold<-ts(train$sold_count,freq=35, start=c(1,1))

both <- cbind(random, fitted)

plot(random, main="Plot of Random Series", col="cyan2")
points(fitted, type = "l", col = "brown2")

plot(ts_train, main="Plot of Log of Actual Series",col="cyan2")
points(fitted_actual_log, type = "l", col = "brown2")

plot(ts_sold, main="Plot of Actual Series",col="cyan2")
points(fitted_actual, type = "l", col = "brown2")

```
The fitted values have captured partly behaviour of the series, however prediction values seem to be overpredicting in peak values of the original data. The model can be improved with adding regressors. For deciding which regressor that should be added, the correlation matrix that contains different attributes should be examined. Looking at the correlations with logarithm of category_favored and price attributes should be chosen.

```{r choose regressors product 4, echo=FALSE, message=FALSE, warning=FALSE}

data <- train[,-c(2,3,4)]
#view(data)
ggcorrplot(corr = cor(data),hc.order = TRUE, type = "upper",lab = TRUE, title = "Correlation Matrix", colors = c("lightgoldenrod1","lightgoldenrod3","lightgoldenrod4"), show.legend = FALSE)


```

The regressors that are chosen are added to the model. It is clear that the new model's AIC and BIC values are much lower, therefore the model is better and it can be proceeded with this model.
```{r model with regressor product 4, echo=FALSE, message=FALSE, warning=FALSE}
xreg <- cbind(train$category_favored, train$price)
model3 <- arima(detrend, order=c(1,0,3), xreg= xreg)
print(model3)
AIC(model3)
BIC(model3)

```


```{r plot model3 product 4, echo=FALSE, message=FALSE, warning=FALSE}
fitted <- random - residuals(model3)
fitted_actual_log <- fitted+trend+seasonal
fitted_actual <- exp(fitted_actual_log)
ts_sold<-ts(train$sold_count,freq=35, start=c(1,1))

both <- cbind(random, fitted)

plot(random, main="Plot of Random Series", col="cyan2")
points(fitted, type = "l", col = "brown2")

plot(ts_train, main="Plot of Log of Actual Series",col="cyan2")
points(fitted_actual_log, type = "l", col = "brown2")

plot(ts_sold, main="Plot of Actual Series",col="cyan2")
points(fitted_actual, type = "l", col = "brown2")

```
The predictions will be made with final model. The predicted values with test set can be seen below. The mean absolute error for each day is plotted and the weighted mean absolute percentage error for this prediction can be seen as well.

```{r prediction product 4, echo=FALSE, message=FALSE, warning=FALSE}
pred_bc<-predict(train$category_favored)
pred_cb<-predict(train$price)
new_category_favored <- pred_bc$fitted[1:7]
new_price <- pred_cb$fitted[1:7]
new_xreg <- cbind(new_category_favored, new_price)
predicted <- predict(model3, newxreg = new_xreg, h=7)$pred
predicted <- ts(predicted,frequency = 7,start=c(1,1))


last_trend <-tail(trend[!is.na(trend)],1)
seasonality <- seasonal[18:24]
predicted_actual_log <- predicted+last_trend+seasonality
predicted_actual <- exp(predicted_actual_log)
(predicted_actual)

```


```{r plotfinal product 4, echo=FALSE, message=FALSE, warning=FALSE}
sevendays <- as.Date(as.Date("2021-06-24"):as.Date("2021-06-30"))

myvalues <- xts(x = data.frame(predicted_actual, test$sold_count), order.by = sevendays, frequency = 7)
colnames(myvalues) <- c("Prediction", "Real Values")
plot(myvalues, main = "Predictions and Real Values", legend.loc = "bottomleft")

```


```{r final product 4, echo=FALSE, message=FALSE, warning=FALSE}
mae <- data.frame( (abs(myvalues$Prediction- test$sold_count)/test$sold_count), sevendays)
names(mae)<- c("MeanAbsoluteErrors", "Date")
plotmae <- ggplot(mae , aes(x= Date , y=MeanAbsoluteErrors)) 

plotmae +  geom_line(color="brown2") +  theme_light()+ labs(title= "Mean Absolute Errors",x= "Days")
weighted_mape <- sum((mae$MeanAbsoluteErrors*100*test$sold_count)/ sum(test$sold_count))

print(paste(" Weighted Mean Absolute Percentage Error : ",weighted_mape))


```
Looking at the plot for Predictions vs Actual Sales, it can be said that the model captured the behaviour of the data partly but it is not the best fitting, also it can be seen on the above plot that there is a peak at Mean Absolute Errors in June 25, this may be an outlier in the data, that may be the reason why the WMAPE is high, in order to solve this issue, further investigation can be made.

# Product 5 - TrendyolMilla Tights

```{r Get Data Product 5, include=FALSE,echo=FALSE}

product5 <- products[[5]]
setDT(product5)

product5$price[product5$price == 0] <- mean(product5$price[product5$price !=0])
product5$basket_count[product5$basket_count == 0] <- mean(product5$basket_count[product5$basket_count !=0])
product5$favored_count[product5$favored_count == 0] <- mean(product5$favored_count[product5$favored_count !=0])
product5$sold_count[product5$sold_count == 0] <- mean(product5$sold_count[product5$sold_count !=0])
product5$visit_count[product5$visit_count == 0] <- mean(product5$visit_count[product5$visit_count !=0])
product5$category_favored[product5$category_favored == 0] <- mean(product5$category_favored[product5$category_favored !=0])
product5$category_sold[product5$category_sold == 0] <- mean(product5$category_sold[product5$category_sold !=0])
product5$category_visits[product5$category_visits == 0] <- mean(product5$category_visits[product5$category_visits !=0])
product5$category_basket[product5$category_basket == 0] <- mean(product5$category_basket[product5$category_basket !=0])
product5$category_brand_sold[product5$category_brand_sold == 0] <- mean(product5$category_brand_sold[product5$category_brand_sold !=0])
product5$ty_visits[product5$ty_visits == 0] <- mean(product5$ty_visits[product5$ty_visits !=0])

train <- product5[product5$event_date <= as.Date("2021-06-23")]
test <- product5[product5$event_date > as.Date("2021-06-23")&product5$event_date <= as.Date("2021-06-30")]



```
First of all, it's clear that the variance of data is very large, from now on, it can be continued with the logarithm of sold count. Below, you can see the actual plot and the plot with the values that are taken logarithms. 
Below, you can see the actual plot and the plot with the values that are taken logarithms. There is a decreasing trend. There may  be seasonality, to look further, there is a plot of 3 months of 2021 - March, April and May. The seasonality isn't easily observed, though it can be said there is a spike in the plot in the middle of the month. In May, there is a big rising probably due to Covid-19 conditions. In conclusion, it can be said that there is monthly seasonality but it isn't very clear.
```{r plot daily product 5, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE}

sold_log <- log(train$sold_count)
train <- cbind(train, sold_log)

ggplot(train,aes(x=event_date,y=sold_count)) + geom_line(color="brown2") +  theme_light()+labs(x = "Date",
       y = "Sales Quantity of the Product 3")

ggplot(train,aes(x=event_date,y=sold_log)) + geom_line(color="brown2") +  theme_light()+labs(x = "Date",
       y = "Log of Sales Quantity of the Product 3") + geom_smooth(color="black", linetype="longdash", se = FALSE)

ggplot(train[event_date<='2021-05-30' & event_date>='2021-03-01'] ,aes(x=event_date,y=sold_log)) + geom_line(color="darkolivegreen4") +  theme_light()+labs(x = "First 3 months of 2021",
       y = "Log of Sales Quantity of the Product 3")

```
30 and 7 day frequency can be selected and the data can be decomposed accordingly. Along with 30 and 7 day frequency, ACF plot of the data can be examined and in the lag that we see high autocorrelation it can be chosen as another trial frequency to decompose. Since variance don't seem to be increasing, additive type of decomposition can be used for decomposition. Below, the random series can be seen.

```{r decomposition weekly product 5, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE}

ts_train<-ts(train$sold_log,freq=7, start=c(1,1))
ts_train_dec<-decompose(ts_train,type="additive")
plot(ts_train_dec)

seasonal<- ts_train_dec$seasonal
deseasonalize<-ts_train-seasonal

tsdeseasonalize <- ts(deseasonalize, start = c(1,1), frequency = 7)
plot1 <- autoplot(tsdeseasonalize, col = "black")+  
  theme_light()
plot3 <- ggAcf(deseasonalize, lag.max=400,  col = "black") + theme_light()

grid.arrange(plot1, plot3)

trend <- ts_train_dec$trend
random <- ts_train_dec$random
detrend<-deseasonalize-trend

tsdetrend <- ts(detrend, start = c(1,1), frequency = 7)
plot2 <- autoplot(tsdetrend, col = "black")+  
  theme_light()

plot4 <- ggAcf(detrend, lag.max=400,  col = "black") +  theme_light()
grid.arrange(plot2, plot4)


```

```{r decomposition monthly product 5, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE}

ts_train<-ts(train$sold_log,freq=30, start=c(1,1))
ts_train_dec<-decompose(ts_train,type="additive")
plot(ts_train_dec)

seasonal<- ts_train_dec$seasonal
deseasonalize<-ts_train-seasonal

tsdeseasonalize <- ts(deseasonalize, start = c(1,1), frequency = 30)
plot1 <- autoplot(tsdeseasonalize, col = "black")+  
  theme_light()
plot3 <- ggAcf(deseasonalize, lag.max=400,  col = "black") + theme_light()

grid.arrange(plot1, plot3)

trend <- ts_train_dec$trend
random <- ts_train_dec$random
detrend<-deseasonalize-trend

tsdetrend <- ts(detrend, start = c(1,1), frequency = 30)
plot2 <- autoplot(tsdetrend, col = "black")+  
  theme_light()

plot4 <- ggAcf(detrend, lag.max=400,  col = "black") +  theme_light()
grid.arrange(plot2, plot4)


```
The above decomposition series belong to time series with 7 and 30 days frequency, respectively.
```{r plot acf product 5, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE}

plotacf <- ggAcf(train$sold_log, lag.max=80,  col = "coral4") + theme_light()
grid.arrange(plotacf)

```

Looking at the ACF plot of the series, highest ACF value belongs to lag 62, so time series decomposition with 62 day frequency would be sufficient.



```{r decomposition product 5, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE}

ts_train<-ts(train$sold_log,freq=62, start=c(1,1))
ts_train_dec<-decompose(ts_train,type="additive")
plot(ts_train_dec)

seasonal<- ts_train_dec$seasonal
deseasonalize<-ts_train-seasonal

tsdeseasonalize <- ts(deseasonalize, start = c(1,1), frequency = 62)
plot1 <- autoplot(tsdeseasonalize, col = "black")+  
  theme_light()
plot3 <- ggAcf(deseasonalize, lag.max=400,  col = "black") + theme_light()

grid.arrange(plot1, plot3)

trend <- ts_train_dec$trend
random <- ts_train_dec$random
detrend<-deseasonalize-trend

tsdetrend <- ts(detrend, start = c(1,1), frequency = 62)
plot2 <- autoplot(tsdetrend, col = "black")+  
  theme_light()

plot4 <- ggAcf(detrend, lag.max=400,  col = "black") +  theme_light()
grid.arrange(plot2, plot4)



```

```{r decomposition weekly product 5 final, fig.align='center', include=FALSE , echo=FALSE, message=FALSE, warning=FALSE}

ts_train<-ts(train$sold_log,freq=7, start=c(1,1))
ts_train_dec<-decompose(ts_train,type="additive")
plot(ts_train_dec)

seasonal<- ts_train_dec$seasonal
deseasonalize<-ts_train-seasonal

tsdeseasonalize <- ts(deseasonalize, start = c(1,1), frequency = 7)
plot1 <- autoplot(tsdeseasonalize, col = "black")+  
  theme_light()
plot3 <- ggAcf(deseasonalize, lag.max=400,  col = "black") + theme_light()

grid.arrange(plot1, plot3)

trend <- ts_train_dec$trend
random <- ts_train_dec$random
detrend<-deseasonalize-trend

tsdetrend <- ts(detrend, start = c(1,1), frequency = 7)
plot2 <- autoplot(tsdetrend, col = "black")+  
  theme_light()

plot4 <- ggAcf(detrend, lag.max=400,  col = "black") +  theme_light()
grid.arrange(plot2, plot4)


```
In this case, the random part of the decomposed time series with 7 day frequency seem to be closer to randomly distributed series with mean zero and std dev 1, so it is chosen as the final decomposition.

```{r choose product 5, echo=FALSE, message=FALSE, warning=FALSE}
plot5 <- ggAcf(detrend, lag.max=15,  col = "maroon1") +  
  theme_light()
plot6 <- ggAcf(detrend, lag.max=15,  col = "maroon1", type=c("partial")) +  
  theme_light()
grid.arrange(plot5, plot6)
```
Looking at the ACF, for 'q' value 1 , 3 or 5 may be selected and looking at the PACF, for 'p' value 2  may be selected. Also, auto.arima function is used as well. The AIC and BIC values of models that are suggested can be seen below. Looking at AIC and BIC values, (2,0,5) model which has been built with respect to observations in ACF and PACF plots is best among them. Also it is better than the model that auto arima has suggested.Arima (2,0,5)'s AIC value is smaller than Arıma(1,0,1) which is suggested b auto arima.


```{r try product 5, echo=FALSE, message=FALSE, warning=FALSE}
library(stats)

model1 <- arima(detrend, order=c(2,0,1))
print(model1)
AIC(model1)
BIC(model1)
model2 <- arima(detrend, order=c(2,0,3))
print(model2)
AIC(model2)
BIC(model2)
model3 <- arima(detrend, order=c(2,0,5))
print(model3)
AIC(model3)
BIC(model3)

final=auto.arima(detrend, seasonal=FALSE, trace=T)
print(final)
AIC(final)
BIC(final)

final = arima(detrend, order=c(2,0,5))

```
Below, there is comparison with training set and fitted values of final model to understand how the final model has learned data. There is plot of random series, plot of logarithm of actual series and plot of actual series. The brown lines belong to fitted model and light blue lines belong to actual series.

```{r plot product 5, echo=FALSE, message=FALSE, warning=FALSE}
fitted <- random - residuals(final)
fitted_actual_log <- fitted+trend+seasonal
fitted_actual <- exp(fitted_actual_log)
ts_sold<-ts(train$sold_count,freq=7, start=c(1,1))

both <- cbind(random, fitted)

plot(random, main="Plot of Random Series", col="cyan2")
points(fitted, type = "l", col = "brown2")

plot(ts_train, main="Plot of Log of Actual Series",col="cyan2")
points(fitted_actual_log, type = "l", col = "brown2")

plot(ts_sold, main="Plot of Actual Series",col="cyan2")
points(fitted_actual, type = "l", col = "brown2")

```

The fitted values have captured the behaviour of the series nicely, however in peak dates for actual sales prediction values seem to be overpredicting in peak values of the original data. The model can be improved with adding regressors. For deciding which regressor that should be added, the correlation matrix that contains different attributes should be examined. Looking at the correlations with logarithm of favored_count and category_sold attributes should be chosen.

```{r choose regressors product 5, echo=FALSE, message=FALSE, warning=FALSE}

data <- train[,-c(2,3,4)]
#view(data)
ggcorrplot(corr = cor(data),hc.order = TRUE, type = "upper",lab = TRUE, title = "Correlation Matrix", colors = c("lightgoldenrod1","lightgoldenrod3","lightgoldenrod4"), show.legend = FALSE)


```

The regressors that are chosen are added to the model. It is clear that the new model's AIC and BIC values are much lower, therefore the model is better and it can be proceeded with this model.
```{r model with regressor product 5, echo=FALSE, message=FALSE, warning=FALSE}
xreg <- cbind(train$favored_count, train$category_sold)
model3 <- arima(detrend, order=c(2,0,5), xreg= xreg)
print(model3)
AIC(model3)
BIC(model3)

```


```{r plot model3 product 5, echo=FALSE, message=FALSE, warning=FALSE}
fitted <- random - residuals(model3)
fitted_actual_log <- fitted+trend+seasonal
fitted_actual <- exp(fitted_actual_log)
ts_sold<-ts(train$sold_count,freq=7, start=c(1,1))

both <- cbind(random, fitted)

plot(random, main="Plot of Random Series", col="cyan2")
points(fitted, type = "l", col = "brown2")

plot(ts_train, main="Plot of Log of Actual Series",col="cyan2")
points(fitted_actual_log, type = "l", col = "brown2")

plot(ts_sold, main="Plot of Actual Series",col="cyan2")
points(fitted_actual, type = "l", col = "brown2")

```

The predictions will be made with final model. The predicted values with test set can be seen below. The mean absolute error for each day is plotted and the weighted mean absolute percentage error for this prediction can be seen as well.
```{r prediction product 5, echo=FALSE, message=FALSE, warning=FALSE}
pred_bc<-predict(train$favored_count)
pred_cb<-predict(train$category_sold)
new_favored_count <- pred_bc$fitted[1:7]
new_category_sold <- pred_cb$fitted[1:7]
new_xreg <- cbind(new_favored_count, new_category_sold)
predicted <- predict(model3, newxreg = new_xreg, h=7)$pred
predicted <- ts(predicted,frequency = 7,start=c(1,1))


last_trend <-tail(trend[!is.na(trend)],1)
seasonality <- seasonal[18:24]
predicted_actual_log <- predicted+last_trend+seasonality
predicted_actual <- exp(predicted_actual_log)
(predicted_actual)

```


```{r plotfinal product 5, echo=FALSE, message=FALSE, warning=FALSE}
sevendays <- as.Date(as.Date("2021-06-24"):as.Date("2021-06-30"))

myvalues <- xts(x = data.frame(predicted_actual, test$sold_count), order.by = sevendays, frequency = 7)
colnames(myvalues) <- c("Prediction", "Real Values")
plot(myvalues, main = "Predictions and Real Values", legend.loc = "bottomleft")

```


```{r final product 5, echo=FALSE, message=FALSE, warning=FALSE}
mae <- data.frame( (abs(myvalues$Prediction- test$sold_count)/test$sold_count), sevendays)
names(mae)<- c("MeanAbsoluteErrors", "Date")
plotmae <- ggplot(mae , aes(x= Date , y=MeanAbsoluteErrors)) 

plotmae +  geom_line(color="brown2") +  theme_light()+ labs(title= "Mean Absolute Errors",x= "Days")
weighted_mape <- sum((mae$MeanAbsoluteErrors*100*test$sold_count)/ sum(test$sold_count))

print(paste(" Weighted Mean Absolute Percentage Error : ",weighted_mape))


```

Looking at the plot for Predictions vs Actual Sales, it can be said that the model captured the behaviour of the data partly but it is not the best fitting, also it can be seen on the above plot that there is a peak at Mean Absolute Errors in June 25 and also June 27,seeing peak at error plot in Jun 25 date for different products may be an indicator for unknown or undefined campaign of Trendyol or event ,in order to solve this issue, further investigation can be made.


# PRODUCT 6 - TrendyolMilla Bikini Top

```{r product3, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE}
product3 <- products[[6]]
setDT(product3)

product3[is.na(price)==T, price:= 0]
product3$price[product3$price == 0] <- mean(product3$price[product3$price !=0])
product3$basket_count[product3$basket_count == 0] <- mean(product3$basket_count[product3$basket_count !=0])
product3$favored_count[product3$favored_count == 0] <- mean(product3$favored_count[product3$favored_count !=0])
product3$sold_count[product3$sold_count == 0] <- mean(product3$sold_count[product3$sold_count !=0])
product3$visit_count[product3$visit_count == 0] <- mean(product3$visit_count[product3$visit_count !=0])
product3$category_favored[product3$category_favored == 0] <- mean(product3$category_favored[product3$category_favored !=0])
product3$category_sold[product3$category_sold == 0] <- mean(product3$category_sold[product3$category_sold !=0])
product3$category_visits[product3$category_visits == 0] <- mean(product3$category_visits[product3$category_visits !=0])
product3$category_basket[product3$category_basket == 0] <- mean(product3$category_basket[product3$category_basket !=0])
product3$category_brand_sold[product3$category_brand_sold == 0] <- mean(product3$category_brand_sold[product3$category_brand_sold !=0])
product3$ty_visits[product3$ty_visits == 0] <- mean(product3$ty_visits[product3$ty_visits !=0])

train <- product3[product3$event_date <= as.Date("2021-06-23")]
test <- product3[product3$event_date > as.Date("2021-06-23") & product3$event_date <= as.Date("2021-06-30")]
#view(train)

```

It should be looked at the plot of data and examined the seasonalities and trend at first. For the empty places in sold counts, the mean of the data is taken. It's clear that the variance of data is very large, from now on, it can be continued with the logarithm of sold count. Below, you can see the actual plot and the plot with the values that are taken logarithms. There is a slightly increasing trend, especially in the beginning and end of the plot. There can't be seen any significant seasonality. To look further, there is a plot of 3 months of 2021 - March, April and May -. Again, the seasonality isn't significant. In conclusion, it can be said that there is no seasonality.


```{r plot3 daily6, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE}

sold_log <- log(train$sold_count)
train <- cbind(train, sold_log)

ggplot(train,aes(x=event_date,y=sold_count)) + geom_line(color="brown2") +  theme_light()+labs(x = "Date",
       y = "Sales Quantity of the Product")

ggplot(train,aes(x=event_date,y=sold_log)) + geom_line(color="brown2") +  theme_light()+labs(x = "Date",
       y = "Log of Sales Quantity of the Product") + geom_smooth(color="black", linetype="longdash", se = FALSE)

ggplot(train[event_date<='2021-05-30' & event_date>='2021-03-01'] ,aes(x=event_date,y=sold_log)) + geom_line(color="darkolivegreen4") +  theme_light()+labs(x = "3 months of 2021",
       y = "Log of Sales Quantity of the Product")

```

Before decomposing, it can be looked to the autocorrelation plot of data. Below, it can be seen that there is a spike at lag 10. We can determine the frequency as 10 because there wasn't significant seasonality.

```{r plot acf6, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE}

plotacf <- ggAcf(train$sold_log, lag.max=40,  col = "coral4") + theme_light()
grid.arrange(plotacf)

```

Now, the data will be decomposed in order to make random series."Additive" type of decomposing will be used because the variance of data is very low due to switching to logarithm. Plots of deseasonalized and the random series their autocorrelations can be seen below.


```{r decomposition6, fig.align='center', echo=FALSE, message=FALSE, warning=FALSE}

ts_train<-ts(train$sold_log,freq=10, start=c(1,1))
ts_train_dec<-decompose(ts_train,type="additive")
plot(ts_train_dec)

seasonal<- ts_train_dec$seasonal
deseasonalize<-ts_train-seasonal

tsdeseasonalize <- ts(deseasonalize, start = c(1,1), frequency = 10)
plot1 <- autoplot(tsdeseasonalize, col = "black")+  
  theme_light()
plot3 <- ggAcf(deseasonalize, lag.max=400,  col = "black") + theme_light()

grid.arrange(plot1, plot3)

trend <- ts_train_dec$trend
random <- ts_train_dec$random
detrend<-deseasonalize-trend

tsdetrend <- ts(detrend, start = c(1,1), frequency = 10)
plot2 <- autoplot(tsdetrend, col = "black")+  
  theme_light()

plot4 <- ggAcf(detrend, lag.max=400,  col = "black") +  theme_light()
grid.arrange(plot2, plot4)


```

For Arima model, (p,d,q) values should be chosen. For this purpose, it can be looked at the ACF and PACF plots. Looking at the ACF, for 'q' value 3 or 10 can be chosen and looking at the PACF, for 'p' value 5 can be chosen.


```{r choose6, echo=FALSE, message=FALSE, warning=FALSE}
plot5 <- ggAcf(detrend, lag.max=15,  col = "maroon1") +  
  theme_light()
plot6 <- ggAcf(detrend, lag.max=15,  col = "maroon1", type=c("partial")) +  
  theme_light()
grid.arrange(plot5, plot6)
```

The AIC and BIC values of models that are suggested can be seen below. Also, auto.arima function is used. So, looking at AIC and BIC values, (5,0,3) model that is suggested above is best among them. We can proceed with this model.


```{r try6, echo=FALSE, message=FALSE, warning=FALSE}
library(stats)

model1 <- arima(detrend, order=c(5,0,10))
print(model1)
AIC(model1)
BIC(model1)
model2 <- arima(detrend, order=c(5,0,3))
print(model2)
AIC(model2)
BIC(model2)

final=auto.arima(detrend, seasonal=FALSE, trace=T)
print(final)
AIC(final)
BIC(final)

```

Below, there is comparison with training set and fitted values of final model to understand how the final model is learned data. There is plot of random series, plot of logarithm of actual series and plot of actual series. The brown lines belong to fitted model and light blue lines belong to actual series.


```{r plot6, echo=FALSE, message=FALSE, warning=FALSE}
fitted <- random - residuals(model2)
fitted_actual_log <- fitted+trend+seasonal
fitted_actual <- exp(fitted_actual_log)
ts_sold<-ts(train$sold_count,freq=10, start=c(1,1))

both <- cbind(random, fitted)

plot(random, main="Plot of Random Series", col="cyan2")
points(fitted, type = "l", col = "brown2")

plot(ts_train, main="Plot of Log of Actual Series",col="cyan2")
points(fitted_actual_log, type = "l", col = "brown2")

plot(ts_sold, main="Plot of Actual Series",col="cyan2")
points(fitted_actual, type = "l", col = "brown2")

```

The model can be improved with adding regressors. For deciding which regressor that should be added, it should be looked the correlation matrix that contains different attributes. Looking at the correlations with logarithm of sold counts, just basket_count attribute should be chosen.

```{r choose regressors6, echo=FALSE, message=FALSE, warning=FALSE}

data3 <- train[,-c(2,3,4)]
ggcorrplot(corr = cor(data3),hc.order = TRUE, type = "upper",lab = TRUE, title = "Correlation Matrix", colors = c("lightgoldenrod1","lightgoldenrod3","lightgoldenrod4"), show.legend = FALSE, lab_size = 3)


```

The regressors that are chosen are added to the model. The new model's AIC and BIC values aren't much lower, there is a little difference betwwen the values. Therefore we can proceed with the previous model that is chosen because there wouldn't be large difference between them.

```{r model with regressor6, echo=FALSE, message=FALSE, warning=FALSE}
xreg <- train$basket_count
model3 <- arima(detrend, order=c(5,0,3), xreg= xreg)
print(model3)
AIC(model3)
BIC(model3)

```

The predictions will be made with final model. The predicted values with test set can be seen below. The mean absolute error for each day is plotted and the weighted mean absolute percentage error for this prediction can be seen as well.


```{r prediction6, echo=FALSE, message=FALSE, warning=FALSE}

predicted <- predict(model2, n.ahead = 7)$pred
predicted <- ts(predicted,frequency = 7,start=c(1,1))


last_trend <-tail(trend[!is.na(trend)],1)
seasonality <- seasonal[6:12]
predicted_actual_log <- predicted + last_trend + seasonality
predicted_actual <- exp(predicted_actual_log)

```


```{r plotfinal6, echo=FALSE, message=FALSE, warning=FALSE}
sevendays <- as.Date(as.Date("2021-06-24"):as.Date("2021-06-30"))

myvalues <- xts(x = data.frame(predicted_actual, test$sold_count), order.by = sevendays, frequency = 7)
colnames(myvalues) <- c("Prediction", "Real Values")
plot(myvalues, main = "Predictions and Real Values", legend.loc = "bottomleft")

```


```{r final6, echo=FALSE, message=FALSE, warning=FALSE}
mae <- data.frame( (abs(myvalues$Prediction- test$sold_count)/test$sold_count), sevendays)
names(mae)<- c("MeanAbsoluteErrors", "Date")
plotmae <- ggplot(mae , aes(x= Date , y=MeanAbsoluteErrors)) 

plotmae +  geom_line(color="brown2") +  theme_light()+ labs(title= "Mean Absolute Errors",x= "Days")
weighted_mape <- sum((mae$MeanAbsoluteErrors*100*test$sold_count)/ sum(test$sold_count))

print(paste(" Weighted Mean Absolute Percentage Error : ",weighted_mape))


```

```{r}
# install the required packages first
require(jsonlite)
require(httr)
require(data.table)
library(lubridate)
library(data.table)
library(dplyr)
library(ggplot2)
library(knitr)
library(tidyr)
library(tidyverse)
library(scales)
library(ggcorrplot)
library(forecast)
library(urca)
library(zoo)
library(reshape)
library(GGally)
library(PerformanceAnalytics)
library(gridExtra)
library(corrplot)


get_token <- function(username, password, url_site){
  
  post_body = list(username=username,password=password)
  post_url_string = paste0(url_site,'/token/')
  result = POST(post_url_string, body = post_body)
  
  # error handling (wrong credentials)
  if(result$status_code==400){
    print('Check your credentials')
    return(0)
  }
  else if (result$status_code==201){
    output = content(result)
    token = output$key
  }
  
  return(token)
}

get_data <- function(start_date='2020-03-20', token, url_site){
  
  post_body = list(start_date=start_date,username=username,password=password)
  post_url_string = paste0(url_site,'/dataset/')
  
  header = add_headers(c(Authorization=paste('Token',token,sep=' ')))
  result = GET(post_url_string, header, body = post_body)
  output = content(result)
  data = data.table::rbindlist(output)
  data[,event_date:=as.Date(event_date)]
  data = data[order(product_content_id,event_date)]
  return(data)
}


send_submission <- function(predictions, token, url_site, submit_now=F){
  
  format_check=check_format(predictions)
  if(!format_check){
    return(FALSE)
  }
  
  post_string="list("
  for(i in 1:nrow(predictions)){
    post_string=sprintf("%s'%s'=%s",post_string,predictions$product_content_id[i],predictions$forecast[i])
    if(i<nrow(predictions)){
      post_string=sprintf("%s,",post_string)
    } else {
      post_string=sprintf("%s)",post_string)
    }
  }
  
  submission = eval(parse(text=post_string))
  json_body = jsonlite::toJSON(submission, auto_unbox = TRUE)
  submission=list(submission=json_body)
  
  print(submission)
  # {"31515569":2.4,"32737302":2.4,"32939029":2.4,"4066298":2.4,"48740784":2.4,"6676673":2.4, "7061886":2.4, "73318567":2.4, "85004":2.4} 
  
  if(!submit_now){
    print("You did not submit.")
    return(FALSE)      
  }
  
  
  header = add_headers(c(Authorization=paste('Token',token,sep=' ')))
  post_url_string = paste0(url_site,'/submission/')
  result = POST(post_url_string, header, body=submission)
  
  if (result$status_code==201){
    print("Successfully submitted. Below you can see the details of your submission")
  } else {
    print("Could not submit. Please check the error message below, contact the assistant if needed.")
  }
  
  print(content(result))
  
}

check_format <- function(predictions){
  
  if(is.data.frame(predictions) | is.data.frame(predictions)){
    if(all(c('product_content_id','forecast') %in% names(predictions))){
      if(is.numeric(predictions$forecast)){
        print("Format OK")
        return(TRUE)
      } else {
        print("forecast information is not numeric")
        return(FALSE)                
      }
    } else {
      print("Wrong column names. Please provide 'product_content_id' and 'forecast' columns")
      return(FALSE)
    }
    
  } else {
    print("Wrong format. Please provide data.frame or data.table object")
    return(FALSE)
  }
  
}

# this part is main code
subm_url = 'http://46.101.163.177'

u_name = "Group3"
p_word = "LaQjwxkIGSGhBrRj"
submit_now = FALSE

username = u_name
password = p_word

token = get_token(username=u_name, password=p_word, url=subm_url)
data = get_data(token=token,url=subm_url)

predictions=unique(data[,list(product_content_id)])
#predictions[,forecast:= c(224,110,122,494,2,474,22,62,56)]
#predictions
#send_submission(predictions, token, url=subm_url, submit_now=T)
```

```{r}
rawdata <- read.csv("C:/Users/DELL/Desktop/ProjectRawData.csv", header = TRUE,sep = ",")
rawdata <- as.data.table(rawdata)
colnames <- c( "price","event_date","product_content_id","sold_count" , "visit_count" , "favored_count" , "basket_count","category_sold",     "category_brand_sold", "category_visits"  ,   "ty_visits"  ,  "category_basket",    "category_favored" )
rawdata <- rawdata[,..colnames]
rawdata <- rawdata[!is.na(sold_count)]
rawdata$event_date <- as.Date(rawdata$event_date, format = "%Y-%m-%d")
#rawdata$product_content_id <- as.factor(rawdata$product_content_id)
#data$product_content_id <- as.factor(data$product_content_id)



rawdata <- rbind(rawdata,data) %>% distinct(event_date, product_content_id, .keep_all = TRUE ) %>% arrange(event_date) %>% as.data.table()
 

forecastdata <- tail(rawdata,9) 
forecastdata[, event_date := as.Date(tail(rawdata,9)$event_date + 1)]

rawdata <- rbind(rawdata,forecastdata) %>% distinct(event_date, product_content_id, .keep_all = TRUE ) %>% arrange(event_date) %>% as.data.table()

 # add  month and weekday  component data 

rawdata[,w_day:= wday(event_date)]
rawdata[,mon:= month(event_date)]

forecastdata[,w_day:= wday(event_date)]
forecastdata[,mon:= month(event_date)]


# campaign 

rawdata[, is_campaign := 0]
rawdata[rawdata$event_date >= as.Date("2021-05-07") & rawdata$event_date <= as.Date("2021-05-09"), is_campaign := 1]
rawdata[rawdata$event_date >= as.Date("2020-06-18") & rawdata$event_date <= as.Date("2020-06-20") , is_campaign := 1]
rawdata[rawdata$event_date >= as.Date("2021-06-18") & rawdata$event_date <= as.Date("2021-06-20") , is_campaign := 1]
rawdata[rawdata$event_date >= as.Date("2020-09-08") & rawdata$event_date <= as.Date("2020-09-12") , is_campaign := 1]
rawdata[rawdata$event_date >= as.Date("2021-09-10") & rawdata$event_date <= as.Date("2021-09-12") , is_campaign := 1]
rawdata[rawdata$event_date >= as.Date("2020-11-09") & rawdata$event_date <= as.Date("2020-11-11") , is_campaign := 1]
rawdata[rawdata$event_date >= as.Date("2020-11-25") & rawdata$event_date <= as.Date("2020-11-29") , is_campaign := 1]
rawdata[rawdata$event_date >= as.Date("2021-03-09") & rawdata$event_date <= as.Date("2021-03-12") , is_campaign := 1]
rawdata[rawdata$event_date >= as.Date("2021-04-27") & rawdata$event_date <= as.Date("2021-04-29") , is_campaign := 1]
rawdata[rawdata$event_date >= as.Date("2021-06-09") & rawdata$event_date <= as.Date("2021-06-10") , is_campaign := 1]
rawdata[rawdata$event_date >= as.Date("2020-12-21") & rawdata$event_date <= as.Date("2020-12-24") , is_campaign := 1]





# create data table for each product

product_id <- unique(rawdata$product_content_id)
products <- list()
for ( i in 1:length(product_id)){
products[[i]] <- as.data.table(rawdata[product_content_id == product_content_id[i]])
}

rawdata <- rawdata[event_date <= "2021-06-30",]

```







```{r}


accu=function(actual,forecast,model){
  model = model
  n=length(actual)
  error=actual-forecast
  mean=mean(actual)
  sd=sd(actual)
  CV=sd/mean
  FBias=sum(error)/sum(actual)
  MAPE=sum(abs(error/actual))/n
  RMSE=sqrt(sum(error^2)/n)
  MAD=sum(abs(error))/n
  MADP=sum(abs(error))/sum(abs(actual))
  WMAPE=MAD/mean
  l=data.frame(model,n,mean,sd,CV,FBias,MAPE,RMSE,MAD,MADP,WMAPE)
  return(l)
}

add_arima=function(data,h,f){
 ts <- ts(data$sold_count,frequency =f )
 add <- decompose(ts, type = "add") 
 fitted <- auto.arima(add$random, seasonal= FALSE)
 seasonal <- add$seasonal[(length(data$sold_count)-f): (length(data$sold_count)-f+h-1)]
 trend <- tail(add$trend[!is.na(add$trend)],1)
 forecastted <- as.numeric(forecast(fitted,h)$mean) + seasonal + trend
 return(forecastted)
}

xreg_add_arima=function(data,h,f, xreg){
 ts <- ts(data$sold_count,frequency =f )
 add <- decompose(ts, type = "add") 
 train_xreg<- xreg[1:length(data$event_date)]
 for_xreg <- xreg[(length(data$event_date))]
 fitted <- auto.arima(add$random, seasonal= FALSE, xreg = train_xreg)
 seasonal <- add$seasonal[(length(data$sold_count)-f): (length(data$sold_count)-f+h-1)]
 trend <- tail(add$trend[!is.na(add$trend)],1)
 forecastted <- as.numeric(forecast(fitted,h,xreg = for_xreg)$mean) + seasonal + trend
 return(forecastted)
}






graph1 <- function(i){
    
g1 <-ggplot(products[[i]], aes( x = event_date, y = sold_count)) + geom_line() + ggtitle(paste("product",i)) +theme_minimal()  
g2 <-ggplot(products[[i]], aes( y = sold_count, fill= factor(w_day))) + geom_boxplot() + 
  ggtitle(paste("product",i))+ theme_minimal() 
g3 <-ggplot(products[[i]], aes( y = sold_count, fill= factor(mon))) + geom_boxplot()+
      ggtitle(paste("product",i))+ theme_minimal() 
g4 <- ggplot(products[[i]], aes( x = sold_count)) + geom_histogram() + ggtitle(paste("product",i)) + theme_minimal()+
        facet_wrap(~factor(w_day))
g5 <-ggplot(products[[i]], aes( x = sold_count )) + geom_histogram() + theme_minimal()+ ggtitle(paste("product",i))+
      facet_wrap(~factor(mon))

print(g1)

grid.arrange(
  g2,g4,g3,g5,
  ncol = 2

)

ACF <- acf( products[[i]]$sold_count, plot = FALSE)
plot(ACF, main = paste("ACF OF Product",i))
PACF <- pacf( products[[i]]$sold_count, plot = FALSE)
plot(PACF, main = paste("PACF OF Product",i))
}


corr_graph <- function(data){
  df1 <- data[, c(4,1,5,6,7,11)]
  df2 <- data[, c(4,8:10,12,13)]
chart.Correlation(df1, pch=19)
chart.Correlation(df2, pch=19)

}


```






# Product 7  -Oral-B Rechargeable ToothBrush 

  The product 7 is the Oral-B Rechargeable ToothBrush and it is not expected to increase depends on season effect the product since it is daily routine product, not particularly needed for some terms, however, it is possible to observe seasonality based on economic conditions and customer purchase habits. 

   The data covers approximately one year of sale information, it is not possible to examine the month seasonality, quarterly, weekly seasonality since it is only include one period. Therefore, the data is examined in frequency 7,14, and 30 days to see if the day of week, fortnight, and day of month seems seasonality. 
   


  The sales of product 7 is plotted in the below during time. The box-plot and histogram of sales is plotted to see if there is difference in distiribution of data in week days and month. Moreover ACF and PACF is plotted to see if there is autocorrelation in data. 



```{r}
i <- 7
product7 <- products[[7]]
graph1(7)


```

By the time garpgh and ACF plot, it can be said that there is a trend in data.And the boxplot and histogram of data shows that there is day and month affect in data, the distibution of data differs. Finally, the ACf and Pacf plot show that there is high auto correlation in lag1 and lag7, 

## Examination of Attributes 

The summary of data is shown below to see nature of data. 

```{r}
summary(product7[,1:13])
```

It can be seen that some attributes has unrealistic behaviour. 
 the more than half of category_basket is equals zero, that is not possible since the basket count and category sold is also should in category basket and they not equal zero when category basket equals. 
 THE TRENDYOL visit is always equals one before the particular date and it shows unusual and unrealistic that trendyol only visited once a date. 
 Also, visit count have similar behaviour, even if it is more inclusive meaning sometimes it is less than basket count and sold count.
 
 


```{r}
corr_graph(product7)

```

By examining the correlation garph and relaibility of data , "price","visit_count",  "basket_count","category_basket" , "ty_visits","is_campaign" are choosen as regressors. 

```{r}
product7[is.na(price)]$price <- mean(product7[!is.na(price)]$price)

product7[,trend := 1:.N]



product7[ty_visits==1, ty_visits:= mean(product7$ty_visits)]


test_dates <- tail(product7,8)$event_date
nextday <- tail(product7,1)
train7 <-  product7[!(event_date %in% test_dates)]
test_dates <- test_dates[1:7]
test7 <- product7[event_date %in% test_dates][1:7]

forecast_ahead <- 1

temp <- tail(product7[,c(1,5,6,7,8,9,10,11,12,13)],15)

test7[,c(1,5,6,7,8,9,10,11,12,13)] <- temp[1:7]


xreg7 <- product7[ , c( "price","visit_count",  "basket_count","category_basket" , "ty_visits","is_campaign")]
xreg7 <- as.matrix(xreg7)

```


## Decomposition and Arima Models 



When arima models is constructed, the auto.arima function is used, and in every day the auto.arima function is runs again. 
the seasonality is TRUE, and frequency is determined as seven by observing ACF and PACF graph. 

Additive Model, Multplive Model, and linear regression model is used for decomposition and get stationary data. 



### Daily Decomposition 


```{r}
i=7
daily <- ts(products[[i]]$sold_count, frequency = 7 )
g1 <- ggsubseriesplot(daily) +
  ylab("daily sales ") +
  ggtitle(" Daily sales")

g2 <- ggtsdisplay(daily) 


grid.arrange(g1,g2)
```
 

```{r}

print("the additive model")
ur.kpss(decompose(daily, type = "add")$random)
print("the multiplicative model")
ur.kpss(decompose(daily, type = "mul")$random)

```
The additive model give more stationary data, therefore, additive decomposition will be used in model construction. 


```{r, out.width= '33%'}



decomposed_daily <- decompose(daily) 

autoplot(decomposed_daily)

acf(decomposed_daily$random, lag = 14, na.action = na.pass)
pacf(decomposed_daily$random, lag = 14, na.action = na.pass)

```



```{r}
random <- ts(decomposed_daily$random, frequency = 7)
fit1 <- auto.arima(random)
fit1

```



### Fortnigth Decomposition


```{r}
fortnight <- ts(products[[i]]$sold_count, frequency = 14 )
ggsubseriesplot(fortnight) +
  ylab("fortnight sales ") +
  ggtitle(" fortnight sales")

ggtsdisplay(fortnight) 
```

```{r}

print("the additive model")
ur.kpss(decompose(fortnight, type = "add")$random)
print("the multiplicative model")
ur.kpss(decompose(fortnight, type = "mul")$random)
```
The data is more stationary than daily decomposition. and addtive method is more stationary. 




```{r, out.width= '33%'}


decomposed_fortnight <- decompose(fortnight) 

autoplot(decomposed_fortnight)

acf(decomposed_fortnight$random, lag = 42, na.action = na.pass)
pacf(decomposed_fortnight$random, lag = 42, na.action = na.pass)
```



```{r}

random <- ts(decomposed_daily$random, frequency = 14)
fit2 <- auto.arima(random)
fit2

```


### Decomposition by frequency = 30



```{r , out.width= '50%'}
monthly <- ts(products[[i]]$sold_count, frequency = 30 )
ggsubseriesplot(monthly) +
  ylab(" monthly ") +
  ggtitle(" monthly")

ggtsdisplay(monthly)
```
```{r}

print("the additive model")
ur.kpss(decompose(monthly, type = "add")$random)
print("the multiplicative model")
ur.kpss(decompose(monthly, type = "mul")$random)
```
The Additive method gave more stationary  data, therefore it is used in model construction. 

```{r, out.width= '33%'}



decomposed_monthly <- decompose(monthly)

autoplot(decomposed_monthly)

acf(decomposed_monthly$random, lag = 90, na.action = na.pass)
pacf(decomposed_monthly$random, lag = 90, na.action = na.pass)
```
 
 
```{r}
random <- ts(decomposed_monthly$random, frequency = 30)
fit3 <- auto.arima(random)
fit3


```
 
 
 The frequency = 30 give better results by comparing AIC values, lower AIC value. 
 
### Model with Regressors
The regressors determined above is  used to improve model. 

```{r}
random <- ts(decomposed_monthly$random, frequency = 30)
reg_arima <- arima(random, order= c(1,0,1), seasonal= c(0,0,2), xreg = xreg7 )
reg_arima

```

 The AIC decreased , the regressors improved the model, the AIC value is lower.  
 
 
### Predictions
 
```{r}




add_arima_forecasted <- c(1:length(test_dates))
reg_add_arima_forecasted <- c(1:length(test_dates))

for(i in 1:length(test_dates)){
    current_date=test_dates[i]-forecast_ahead
    past_data=product7[event_date<=current_date]
    forecast_data=product7[event_date==test_dates[i]]
    
    
    add_arima_forecasted[i] <- add_arima(past_data,1,30)
    reg_add_arima_forecasted[i] <- xreg_add_arima(past_data,1,30,xreg7)
}
    
 
 testprediction7 <- data.table("event_date" = test_dates, "actual"= product7[event_date %in% test_dates]$sold_count, add_arima_forecasted,reg_add_arima_forecasted)
 
 testprediction7


ggplot() + geom_line(data = product7[event_date %in% test_dates], aes(x= event_date, y = sold_count)) + 
  geom_line(data = testprediction7,aes( x = event_date,y = add_arima_forecasted, color = "add_arima_forecasted"))+ 
   geom_line(data = testprediction7,aes( x = event_date,y = reg_add_arima_forecasted, color = "reg_add_arima_forecasted"))








```

### Model evaluation 

 The error rates shows the arima model with regressors have gave better results by considering WMAPE
 
```{r}


for(i in 3:length(testprediction7)){
  
  print(accu(test7$sold_count,testprediction7[,..i],colnames(testprediction7[,..i])))
  

}
```



# Product 8  - Altinyildiz Classics Jacket


   The data covers approximately one year of sale information, it is not possible to examine the month seasonality, quarterly, weekly seasonality since it is only include one period. Therefore, the data is examined in frequency 7,14, and 30 days to see if the day of week, fortnight, and day of month seems seasonality. 
   


  The sales of product 8 is plotted in the below during time. The box-plot and histogram of sales is plotted to see if there is difference in distiribution of data in week days and month. Moreover ACF and PACF is plotted to see if there is autocorrelation in data. 

  The Product8 is  a Jacket and it show incearsing in sales for some months. The temparature of season can have effect on it. 

```{r}
i=8
product8 <- products[[8]]
graph1(8)
```


It can be seen that the sales is zero most of time, however, there is huge increase in October. 

The ACF and PACF of data shows that there is significant autocorrelation in lag1, lag5, lag7 and lag20. 


## Decomposition and Arima Models 




When arima models is constructed, the auto.arima function is used, and in every day the auto.arima function is runs again. 
the seasonality is TRUE, and frequency is determined as seven by observing ACF and PACF graph. 

Additive Modeland  Multplive Model are is used for decomposition and get stationary data. 


### Daily Decomposition 


```{r}

daily <- ts(products[[i]]$sold_count, frequency = 7 )
g1 <- ggsubseriesplot(daily) +
  ylab("daily sales ") +
  ggtitle(" Daily sales")

g2 <- ggtsdisplay(daily) 


grid.arrange(g1,g2)
```



the data is shown decomposed by additive method and multiplicative method. 

```{r}

print("the additive model")
ur.kpss(decompose(daily, type = "add")$random)
print("the multiplicative model")
ur.kpss(decompose(daily, type = "mul")$random)

```

The multiplicative method gave more stationary data and the model constructed by addtive method. 

```{r, out.width= '33%'}



decomposed_daily <- decompose(daily) 

autoplot(decomposed_daily)

acf(decomposed_daily$random, lag = 14, na.action = na.pass)
pacf(decomposed_daily$random, lag = 14, na.action = na.pass)

```



```{r}
random <- ts(decomposed_daily$random, frequency = 7)
fit1 <- auto.arima(random)
fit1

```



### Fortnigth Decomposition


```{r, out.width= '50%'}
i=8
fortnight <- ts(products[[i]]$sold_count, frequency = 14 )
ggsubseriesplot(fortnight) +
  ylab("fortnight sales ") +
  ggtitle(" fortnight sales")

ggtsdisplay(fortnight) 
```


```{r}
print("the additive model")
ur.kpss(decompose(fortnight, type = "add")$random)
print("the multiplicative model")
ur.kpss(decompose(fortnight, type = "mul")$random)
```
The addtive model gave more stationary data, therefore it is used for model. 

```{r, out.width= '33%'}


decomposed_fortnight <- decompose(fortnight) 

autoplot(decomposed_fortnight)

acf(decomposed_fortnight$random, lag = 14, na.action = na.pass)
pacf(decomposed_fortnight$random, lag = 14, na.action = na.pass)

```


```{r}
random <- ts(decomposed_fortnight$random, frequency = 14)
fit2 <- auto.arima(random)
fit2

```


## Decompostion by frequency = 30 



```{r, out.width= '50%'}

monthly <- ts(products[[i]]$sold_count, frequency = 30 )
ggsubseriesplot(monthly) +
  ylab(" monthly ") +
  ggtitle(" monthly")

ggtsdisplay(monthly) 


```


```{r}



print("the additive model")
ur.kpss(decompose(monthly, type = "add")$random)
print("the multiplicative model")
ur.kpss(decompose(monthly, type = "mul")$random)

```

The additive model is used for model construction since it is more stationary 

```{r, out.width= '33%'}
decomposed_monthly <- decompose(monthly) 

autoplot(decomposed_monthly)

acf(decomposed_monthly$random, lag = 60 , na.action = na.pass)
pacf(decomposed_monthly$random, lag = 60, na.action = na.pass)


``` 



```{r}
random= ts(decomposed_monthly$random, frequency = 30)
fit3 <- auto.arima(random)
fit3
```
ARIMA(0,0,1)(0,0,1)[30] with zero mean fives the best AIC result by comparing frequenty 7 and 14. Therefore it is used for predcitions and regressor model.   


## the Examination of Attirubutes

```{r}
summary(product8)

corr_graph(product8)
```


the correlation of price, visit_count, and basket_count is high and it is expected if the sold_count is zero this variables can be zero. 

However, it is not expected that category favored and trendyol visits is zero or one therefore these variables changed as mean. 




```{r}





product8[is.na(price)]$price <- mean(product8[(!is.na(price)) & (price>= 0) ]$price)
product8[price<=0]$price <- mean(product8[!is.na(price)]$price)

product8[ty_visits ==1 , ty_visits:= round(mean(product8$ty_visits))]
product8[category_basket==0,  category_basket:= round(mean(product8$category_basket))]
product8[category_basket > 469103  ,  category_basket:= round(mean(product8$category_basket))]


```



By considering correlation and variable relaibility the "price","visit_count",  "basket_count","category_favored"  are selected as regressors. And, the graph of monthly distirbution shows that mon can be effective factor, therefore, it is also added. 



```{r}
test_dates <- tail(product8,9)$event_date
nextday <- tail(product8,1)
train8 <-  product8[!(event_date %in% test_dates)]
test_dates <- test_dates[1:7]
test8 <- product8[event_date %in% test_dates][1:7]



forecast_ahead <- 1

temp <- tail(product8[,c(1,5,6,7,8,9,10,11,12,13)],15)

test8[,c(1,5,6,7,8,9,10,11,12,13)] <- temp[1:7]



xreg8 <- product8[ , c( "price","visit_count",  "basket_count","category_favored","mon" )]
xreg8 <- as.matrix(xreg8)



```


### arima model with regressors 


```{r}
random <- ts(decomposed_monthly$random, frequency = 30)
reg_arima <- arima(random, order= c(0,0,1), seasonal = c(0,0,1), xreg = xreg8 )
reg_arima
```
the AIC got smaller when regressors added to model, and improved the model. 


### Predictions


```{r}
for(i in 1:length(test_dates)){
    current_date=test_dates[i]-forecast_ahead
    past_data=product8[event_date<=current_date]
    forecast_data=product8[event_date==test_dates[i]]
    
    
  add_arima_forecasted[i] <- round(add_arima(past_data,1,7),0)
  reg_add_arima_forecasted[i] <- round(xreg_add_arima(past_data,1,7,xreg8),0)
    
}


testprediction8 <- data.table("event_date" = test_dates,"actual" = product8[event_date %in% test_dates]$sold_count, add_arima_forecasted,reg_add_arima_forecasted)

testprediction8

ggplot() + geom_line(data = product8[event_date %in% test_dates], aes(x= event_date, y = sold_count)) + 
  geom_line(data = testprediction8,aes( x = event_date,y = add_arima_forecasted, color = "add_arima_forecasted"))+ 
   geom_line(data = testprediction8,aes( x = event_date,y = reg_add_arima_forecasted, color = "reg_add_arima_forecasted"))
```
## Model Evaluation 

```{r}
for(i in 3:length(testprediction8)){
  
  print(accu(test8$sold_count,testprediction8[,..i],colnames(testprediction8[,..i])))
}

```

The arima with regressors and arima model show the same performance in prediction data. 


# Product 9 - TrendyolMilla Bikini 


   The data covers approximately one year of sale information, it is not possible to examine the month seasonality, quarterly, weekly seasonality since it is only include one period. Therefore, the data is examined in frequency 7,14, and 30 days to see if the day of week, fortnight, and day of month seems seasonality. 
   


  The sales of product 8 is plotted in the below during time. The box-plot and histogram of sales is plotted to see if there is difference in distiribution of data in week days and month. Moreover ACF and PACF is plotted to see if there is autocorrelation in data. 


By observing the graph below, the month effect is clearly observable. It is expected since bikini is wore in hot seasons in Turkey. 
Moreover, by examined the acf and pacf graph, it can be said that there is trend in data and correlation with lag1 and lag7. 

```{r}
i=9
product9 <- products[[9]]
graph1(9)
```


## Examination of Attributes



```{r}



summary(product9)

corr_graph(product9)

```


the "price","category_sold",  "basket_count","category_favored" attributes are more relaible and significantly corralet with data. Even if the visit_count and favored_count is very high corralled with data, they also corraleted with basket_count therefore they do not used as regressors. And "mon" is seem to affect on sales by th monthly distribution graph. 



```{r}

product9[is.na(price)]$price <- mean(product9[(!is.na(price)) & (price>= 0) ]$price)
product9[price<=0]$price <- mean(product9[!is.na(price)]$price)



product9[,trend := 1:.N]

product9[ty_visits==1, ty_visits:= mean(product9$ty_visits)]


```





```{r}

test_dates <- tail(product9,9)$event_date
nextday <- tail(product9,1)
train9 <-  product9[!(event_date %in% test_dates)]
test_dates <- test_dates[1:7]
test9 <- product9[event_date %in% test_dates][1:7]



forecast_ahead <- 1



temp <- tail(product9[,c(1,5,6,7,8,9,10,11,12,13)],15)

test9[,c(1,5,6,7,8,9,10,11,12,13)] <- temp[1:7]


xreg9 <- product9[ , c( "price","category_sold",  "basket_count","category_favored","mon" )]
xreg9 <- as.matrix(xreg9)

```




## Decomposition and Arima Models 


When arima models is constructed, the auto.arima function is used, and in every day the auto.arima function is runs again. 
the seasonality is TRUE, and frequency is determined as seven by observing ACF and PACF graph. 

Additive Model, Multplive Model, and linear regression model is used for decomposition and get stationary data. 


### Daily Decomposition 


```{r, out.width= '50%'}
i=9
daily <- ts(products[[i]]$sold_count, frequency = 7 )
g1 <- ggsubseriesplot(daily) +
  ylab("daily sales ") +
  ggtitle(" Daily sales")

g2 <- ggtsdisplay(daily) 


grid.arrange(g1,g2)
```



```{r}

print("the additive model")
ur.kpss(decompose(daily, type = "add")$random)
print("the multiplicative model")
ur.kpss(decompose(daily, type = "mul")$random)

```

The addtive method give more stationary results therefore it is used for models. 

```{r, out.width= '33%'}



decomposed_daily <- decompose(daily) 

autoplot(decomposed_daily)

acf(decomposed_daily$random, lag = 14, na.action = na.pass)
pacf(decomposed_daily$random, lag = 14, na.action = na.pass)

```

```{r}
random <- ts(decomposed_daily$random, frequency = 7)
fit1 <- auto.arima(random)
fit1

```



### Fortnigth Decomposition


```{r, out.width= '50%'}
fortnight <- ts(products[[i]]$sold_count, frequency = 14 )
ggsubseriesplot(fortnight) +
  ylab("fortnight sales ") +
  ggtitle(" fortnight sales")

ggtsdisplay(fortnight) 
```


```{r}
print("the additive model")
ur.kpss(decompose(fortnight, type = "add")$random)
print("the multiplicative model")
ur.kpss(decompose(fortnight, type = "mul")$random)
```
the additive model give more stationary results and it is used for model construction. 

```{r, out.width= '33%'}


decomposed_fortnight <- decompose(fortnight) 

autoplot(decomposed_fortnight)

acf(decomposed_fortnight$random, lag = 14, na.action = na.pass)
pacf(decomposed_fortnight$random, lag = 14, na.action = na.pass)

```


```{r}
random <- ts(decomposed_fortnight$random, frequency = 14)
fit2 <- auto.arima(random)
fit2

```


## Decompostion by frequency = 30 



```{r, out.width= '50%'}

monthly <- ts(products[[i]]$sold_count, frequency = 30 )
ggsubseriesplot(monthly) +
  ylab(" monthly ") +
  ggtitle(" monthly")

ggtsdisplay(monthly) 

```


```{r}



print("the additive model")
ur.kpss(decompose(monthly, type = "add")$random)
print("the multiplicative model")
ur.kpss(decompose(monthly, type = "mul")$random)

```
 "the additive model" give more stationary data, therefore it is used as model data. 


```{r}
decomposed_monthly <- decompose(monthly) 

autoplot(decomposed_monthly)

acf(decomposed_monthly$random, lag = 60 , na.action = na.pass)
pacf(decomposed_monthly$random, lag = 60, na.action = na.pass)


``` 



```{r}
random= ts(decomposed_monthly$random, frequency = 30)
fit3 <- auto.arima(random)
fit3
```

ARIMA(0,0,2)(0,0,2)[7] with zero mean gives the best AIC score, and it is used for arima regressor model. 


### Model with Regressors

```{r}
random <- ts(decomposed_monthly$random, frequency = 7)
reg_arima <- arima(random, order= c(0,0,2), seasonal = c(0,0,2), xreg = xreg9 )
reg_arima

```

the AIC value is lower than arima mode, so it is improved model. 

### Predictions

```{r}

 add_arima_forecasted <- c(1:length(test_dates))
 reg_add_arima_forecasted <- c(1:length(test_dates))

for(i in 1:length(test_dates)){
    current_date=test_dates[i]-forecast_ahead
    past_data=product9[event_date<=current_date]
    forecast_data=product9[event_date==test_dates[i]]
    
    
    add_arima_forecasted[i] <- add_arima(past_data,1,7)
    reg_add_arima_forecasted[i] <- xreg_add_arima(past_data,1,7,xreg9)

}

 testprediction9 <- data.table("event_date" = test_dates, "Actual" =  product9[event_date %in% test_dates]$sold_count, add_arima_forecasted,reg_add_arima_forecasted)
 
 testprediction9

ggplot() + geom_line(data = product9[event_date %in% test_dates], aes(x= event_date, y = sold_count)) + 
  geom_line(data = testprediction9,aes( x = event_date,y = add_arima_forecasted, color = "add_arima_forecasted"))+ 
   geom_line(data = testprediction9,aes( x = event_date,y = reg_add_arima_forecasted, color = "reg_add_arima_forecasted"))



```
## Model Evaluation


```{r}

for(i in 3:length(testprediction9)){
  
  print(accu(test9$sold_count,testprediction9[,..i],colnames(testprediction9[,..i])))
}

```

The arima model with no regressors gives better result in test data. 


# CONCLUSION


In order to find best decomposition level and with respect to that find the best ARIMA models for different products, different decomposition levels have tried and selected, then ARIMA models have been tried and their performance on the test set have been measured, which consists of dates from 24 June 2021 to 30 June 2021, different models have been selected for each product.

Since sales are affected from the overall component of the economy, so more external data could be included such as dollar exchange rate, for improved accuracy.

Approaching differently to each product is one of the strong sides of the model, since it is a time consuming task. Also comparing AIC values for the model auto arima suggested with the models that have been selected with respect to ACF and PACF plots and measuring their performances based on their predictions on the test data is also a strong side of the models that have been proposed for each product.

 Overall, it can be said that models work fine, deviation from the real values is not too big.

# REFERENCES

Lecture Notes

# RMD 
 The code of my study is available from [here](https://bu-ie-360.github.io/spring21-oyahobann/files/HW4-5.Rmd)


