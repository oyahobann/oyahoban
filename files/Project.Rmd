
# IE 425 TERM PROJECT
## Özlem Şenel - 2017402117
## Oya Hoban - 2018402150

```{r setup, include=FALSE}

library(caret)
library(randomForest)
library(caTools)
library(gbm)
library(data.table)
library(ISLR)
library(boot)
require(jsonlite)
require(httr)
library(lubridate)
library(data.table)
library(dplyr)
library(ggplot2)
library(knitr)
library(tidyr)
library(tidyverse)
library(scales)
library(ggcorrplot)
library(forecast)
library(urca)
library(zoo)
library(reshape)
library(GGally)
library(PerformanceAnalytics)
library(ROCR)
library(gbm)
library("Metrics")
library("pROC")

set.seed(425)

org_train_data <- read.csv("C:/Users/DELL/Desktop/2020-2021 Ders 2/IE 425/Proje/kobe-train.csv", header = TRUE,sep = ",")
org_test_data <-  read.csv("C:/Users/DELL/Desktop/2020-2021 Ders 2/IE 425/Proje/kobe-test.csv", header = TRUE,sep = ",")

train_data <- read.csv("C:/Users/DELL/Desktop/2020-2021 Ders 2/IE 425/Proje/kobe-train.csv", header = TRUE,sep = ",")

setDT(train_data)
setDT(org_train_data)
setDT(org_test_data)

```

## INTRODUCTION

In this project we should use the given data which is about Kobe Bryant's basketball career. It contains the location and circumstances of every field goal attempted by Kobe Bryant during his career. We are trying to predict whether the shot made flag. In order to achieve this goal, we split the train data into train and test sets, and try different methods to find the best one in terms of Area Under the Curve (AUC) values. 


## DATA MANIPULATION

Firstly, we can observe that there is a large amount of input attributes in the data. It would be wise that we try to eliminate some of the attributes that may indicate close concepts. We see that there are two parameters for x and y coordinates, taking their euclidean distance from (0,0) location would reduce our number of attributes.

```{r Manipulation of the Data Part1, echo = FALSE}

train_data[,shot_made_flag:=as.factor(train_data$shot_made_flag)]

plot <- ggplot(data = train_data ,aes(color = shot_made_flag,y = loc_y, x= loc_x))
plot + geom_point()  +  theme_light()

train_data[,loc:= sqrt(loc_x^2 + loc_y^2)]


head(data.frame(train_data$loc_x, train_data$loc_y, train_data$loc))
train_data <- train_data[,-c("loc_x","loc_y")]
```



Secondly, we can combine the minutes and seconds remaining into a times remaining attribute. Also, team ID and team name attributes that have one level, so we can exclude them from the data.

{r Manipulation of the Data Part2, echo = FALSE}

train_data[,time_remaining := (minutes_remaining*60 + seconds_remaining)]
head(data.frame(train_data$minutes_remaining,train_data$seconds_remaining))

train_data <- train_data[,-c("minutes_remaining","seconds_remaining")]
train_data <- train_data[,-c("team_name","team_id")]




Another attribute that we can inspect is matchup attribute. Matchup includes whether the game is at home or at opponent's home. As opponent attribute is already in the data, we can reduce the matchup attribute to 2 levels which 'ishome' or not. 



{r  Manipulation of the Data Part3, echo = FALSE}
train_data[grepl("@", train_data$matchup, fixed=TRUE)==TRUE, ishome:=1]
train_data[is.na(train_data$ishome)==TRUE, ishome:=0]
head(data.frame(train_data$matchup, train_data$ishome))
#str(train_data)
train_data <- train_data[,-c("matchup")]



The data still has many input attributes. Game Date is one of the attributes which has large amount of levels. As we can see on the table, season is more aggregated version of game date, so we can discard game date and use season for shorter run time of the model.

{r Manipulation of the Data Part4, echo = FALSE}
df <- data.frame(train_data$game_date,train_data$season)
df

train_data <- train_data[,-c("game_date")]




{r  asfactor, echo = FALSE}

train_data[,shot_made_flag:=as.factor(train_data$shot_made_flag)]

train_data[,action_type:=as.factor(train_data$action_type)]
train_data[,season:=as.factor(train_data$season)]
train_data[,shot_type:=as.factor(train_data$shot_type)]
train_data[,shot_zone_area:=as.factor(train_data$shot_zone_area)]
train_data[,shot_zone_basic:=as.factor(train_data$shot_zone_basic)]
train_data[,shot_zone_range:=as.factor(train_data$shot_zone_range)]

train_data[,game_date:=as.factor(train_data$game_date)]

train_data[,opponent:=as.factor(train_data$opponent)]
train_data[,combined_shot_type:=as.factor(train_data$combined_shot_type)]
train_data[,game_id:=as.factor(train_data$game_id)]

split=sample.split(train_data$shot_made_flag,SplitRatio=0.7)

test_data=subset(train_data,split==FALSE)
train_data=subset(train_data,split==TRUE)

train_data <- train_data[,-c("X")]


## MODELS


We can try different models to decide on the best model that we can use for our final prediction on the given test set. So, we search for the largest value for Area Under the Curve.

### Modeling with Rpart

We use caret package for our first model. The best cp value for rpart with 10-fold cross validation model is 0.002. The accuracy value is 0.6762627 and AUC value is 0.6643. 


{r prediction with rpart, echo = FALSE}
ctrl1=trainControl(method='cv',number=10)
fit1=train(shot_made_flag~., data= train_data, method = "rpart",
           trControl = ctrl1, tuneGrid = expand.grid(cp=(1:10)*0.001))
#best cp = 0.002

fit1

# Accuracy value = 0.6762627
pred1 <- predict(fit1, newdata=test_data,type="raw")


predictions1= as.numeric(pred1)

auc(roc(test_data$shot_made_flag, predictions1))

#AUC value = 0.6643

confusionMatrix(as.factor(pred1) ,as.factor(test_data$shot_made_flag), positive="1")

pred1 <- prediction(predictions1,test_data$shot_made_flag)
perf=performance(pred1,"tpr","fpr")
plot(perf)
as.numeric(performance(pred1,"auc")@y.values)




### Modeling with GBM

For second model, we use caret package as well. Accuracy was used to select the optimal model using the largest value. The final values used for the model were n.trees = 60, interaction.depth = 4, shrinkage = 0.2 and n.minobsinnode = 20 and Accuracy is 0.6790396. For this model, AUC value is 0.6666.

{r prediction with rpart, echo = FALSE}
ctrl2 = trainControl(method = "cv", number = 10)

gbmGrid1=expand.grid(interaction.depth = c(3, 4), 
                    n.trees = c(50, 60), 
                    shrinkage = (1:2)*0.1,
                    n.minobsinnode = c(10, 20))


fit2=train(shot_made_flag~., data=train_data, method="gbm", metric='Accuracy',
             trControl = ctrl2,tuneGrid = gbmGrid1)
fit2


pred2 <- predict(fit2, newdata=test_data,type="raw")


predictions2= as.numeric(pred2)

auc(roc(test_data$shot_made_flag, predictions2))

confusionMatrix(as.factor(pred2) ,as.factor(test_data$shot_made_flag), positive="1")

pred2 <- prediction(predictions2,test_data$shot_made_flag)
perf2=performance(pred2,"tpr","fpr")
plot(perf2)
as.numeric(performance(pred2,"auc")@y.values)



### Modeling with GLM

Because action type and game ID attributes have large amount of levels, when we split the given train data, some levels aren't split equally. So, we can disregard these levels for only testing purposes.The AUC value for GLM model is 0.6639954.

{r prediction with rpart, echo = FALSE}
fit3=glm(shot_made_flag~.,data=train_data,family=binomial(link = "logit"))

fit3

test_datav2 <- test_data[action_type != "Driving Floating Bank Jump Shot",]
test_datav2 <- test_datav2[action_type != "Running Slam Dunk Shot",]
test_datav2 <- test_datav2[action_type != "Running Tip Shot",]
test_datav2 <- test_datav2[action_type != "Turnaround Finger Roll Shot",]
test_datav2 <- test_datav2[game_id != "29600095",]
test_datav2 <- test_datav2[game_id != "29600362",]
test_datav2 <- test_datav2[game_id != "29600382",]


pred3 <- predict(fit3, newdata=test_datav2,type="response")


predictions3= as.numeric(pred3)

auc(roc(test_datav2$shot_made_flag, predictions3))

#AUC = 0.6639954



pred3 <- prediction(predictions3,test_datav2$shot_made_flag)
perf3=performance(pred3,"tpr","fpr")
plot(perf3)
as.numeric(performance(pred3,"auc")@y.values)